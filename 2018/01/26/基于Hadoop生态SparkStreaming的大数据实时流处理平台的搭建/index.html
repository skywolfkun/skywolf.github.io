<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="大数据,Hadoop,Spark,SparkStreaming,kafka,HBase,流处理" />








  <link rel="shortcut icon" type="image/x-icon" href="/wolf.ico?v=5.1.0" />






<meta name="description" content="随着公司业务发展，对大数据的获取和实时处理的要求就会越来越高，日志处理、用户行为分析、场景业务分析等等，传统的写日志方式根本满足不了业务的实时处理需求，所以本人准备开始着手改造原系统中的数据处理方式，重新搭建一个实时流处理平台，主要是基于Hadoop生态，利用Kafka作为中转，SparkStreaming框架实时获取数据并清洗，将结果多维度的存储进HBase数据库。　整个平台大致的框架如下：">
<meta property="og:type" content="article">
<meta property="og:title" content="基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建">
<meta property="og:url" content="http://www.tianshangkun.com/2018/01/26/基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建/index.html">
<meta property="og:site_name" content="天狼武士的Blog">
<meta property="og:description" content="随着公司业务发展，对大数据的获取和实时处理的要求就会越来越高，日志处理、用户行为分析、场景业务分析等等，传统的写日志方式根本满足不了业务的实时处理需求，所以本人准备开始着手改造原系统中的数据处理方式，重新搭建一个实时流处理平台，主要是基于Hadoop生态，利用Kafka作为中转，SparkStreaming框架实时获取数据并清洗，将结果多维度的存储进HBase数据库。　整个平台大致的框架如下：">
<meta property="og:image" content="http://www.tianshangkun.com/images/streaming.png">
<meta property="og:image" content="http://www.tianshangkun.com/images/hbaseInHadoop.png">
<meta property="og:updated_time" content="2018-05-16T08:21:42.681Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建">
<meta name="twitter:description" content="随着公司业务发展，对大数据的获取和实时处理的要求就会越来越高，日志处理、用户行为分析、场景业务分析等等，传统的写日志方式根本满足不了业务的实时处理需求，所以本人准备开始着手改造原系统中的数据处理方式，重新搭建一个实时流处理平台，主要是基于Hadoop生态，利用Kafka作为中转，SparkStreaming框架实时获取数据并清洗，将结果多维度的存储进HBase数据库。　整个平台大致的框架如下：">
<meta name="twitter:image" content="http://www.tianshangkun.com/images/streaming.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.tianshangkun.com/2018/01/26/基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建/"/>





  <title> 基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建 | 天狼武士的Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  











  <div style="display: none;">
    <script src="//s95.cnzz.com/z_stat.php?id=1261718062&web_id=1261718062" language="JavaScript"></script>
  </div>






  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">天狼武士的Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">世界太大我想敲两行代码</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.tianshangkun.com/2018/01/26/基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="天狼武士">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天狼武士的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-26T16:53:54+08:00">
                2018-01-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/01/26/基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建/" class="leancloud_visitors" data-flag-title="基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计</span>
                
                <span title="字数统计">
                  2,687 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长</span>
                
                <span title="阅读时长">
                  13 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>　随着公司业务发展，对大数据的获取和实时处理的要求就会越来越高，日志处理、用户行为分析、场景业务分析等等，传统的写日志方式根本满足不了业务的实时处理需求，所以本人准备开始着手改造原系统中的数据处理方式，重新搭建一个实时流处理平台，主要是基于Hadoop生态，利用Kafka作为中转，SparkStreaming框架实时获取数据并清洗，将结果多维度的存储进HBase数据库。<br>　整个平台大致的框架如下：<br>　<img src="/images/streaming.png"></p>
<p>　操作系统：Centos7<br>　用到的框架：<br>　1. <a href="http://flume.apache.org/" target="_blank" rel="external">Flume1.8.0</a><br>　2. <a href="http://hadoop.apache.org/" target="_blank" rel="external">Hadoop2.9.0</a><br>　3. <a href="http://kafka.apache.org/" target="_blank" rel="external">kafka2.11-1.0.0</a><br>　4. <a href="spark.apache.org">Spark2.2.1</a><br>　5. <a href="hbase.apache.org">HBase1.2.6</a><br>　6. <a href="http://zookeeper.apache.org/" target="_blank" rel="external">ZooKeeper3.4.11</a><br>　7. <a href="http://maven.apache.org/" target="_blank" rel="external">maven3.5.2</a><br>　整体的开发环境是基于JDK1.8以上以及Scala，所以得提前把java和Scala的环境给准备好，接下来就开始着手搭建基础平台：</p>
<h2 id="一、配置开发环境"><a href="#一、配置开发环境" class="headerlink" title="一、配置开发环境"></a>一、配置开发环境</h2><p>　下载并解压JDK1.8,、下载并解压Scala，配置profile文件：</p>
<pre><code>vim /etc/profile
</code></pre><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=<span class="regexp">/usr/java</span><span class="regexp">/jdk1.8.0_144</span><br><span class="line">export PATH=$JAVA_HOME/bin</span>:$PATH</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/<span class="class"><span class="keyword">lib</span>/<span class="title">dt</span>.<span class="title">jar</span>:$<span class="title">JAVA_HOME</span>/<span class="title">lib</span>/<span class="title">tools</span>.<span class="title">jar</span></span></span><br><span class="line">export SCALA_HOME=<span class="regexp">/usr/local</span><span class="regexp">/scala-2.11.12</span><br><span class="line">export PATH=$PATH:$SCALA_HOME/bin</span></span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><h2 id="二、配置zookeeper、maven环境"><a href="#二、配置zookeeper、maven环境" class="headerlink" title="二、配置zookeeper、maven环境"></a>二、配置zookeeper、maven环境</h2><p>　下载并解压zookeeper以及maven并配置profile文件</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz
tar -zxvf apache-maven-3.5.2-bin.tar.gz -C /usr/local
wget http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz
tar -zxvf zookeeper-3.4.11.tar.gz -C /usr/local
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> MAVEN_HOME=/usr/<span class="built_in">local</span>/apache-maven-3.5.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$MAVEN_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><p>　zookeeper的配置文件配置一下：</p>
<pre><code>cp /usr/local/zookeeper-3.4.11/conf/zoo_sample.cfg /usr/local/zookeeper-3.4.11/conf/zoo.cfg
</code></pre><p>　然后配置一下zoo.cfg里面的相关配置，指定一下dataDir目录等等<br>　启动zookeeper:</p>
<pre><code>/usr/local/zookeeper-3.4.11/bin/zkServer.sh start
</code></pre><p>　如果不报错，jps看一下是否启动成功</p>
<h2 id="三、安装配置Hadoop"><a href="#三、安装配置Hadoop" class="headerlink" title="三、安装配置Hadoop"></a>三、安装配置Hadoop</h2><p>　Hadoop的安装配置在之前文章中有说过（<a href="/2017/06/13/Centos下docker搭建Hadoop集群/">传送门</a>），为了下面的步骤方便理解，这里只做一个单机版的简单配置说明：<br>　下载hadoop解压并配置环境：</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.9.0/hadoop-2.9.0.tar.gz
tar -zxvf hadoop-2.9.0.tar.gz -C /usr/local
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop-2.9.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><p>　配置hadoop 进入/usr/local/hadoop-2.9.0/etc/hadoop目录</p>
<pre><code>cd /usr/local/hadoop-2.9.0/etc/hadoop
</code></pre><p>　首先配置hadoop-env.sh、yarn-env.sh，修改JAVA_HOME到指定的JDK安装目录/usr/local/java/jdk1.8.0_144<br>　创建hadoop的工作目录</p>
<pre><code>mkdir /opt/data/hadoop
</code></pre><p>　编辑core-site.xml、hdfs-site.xml、yarn-site.xml等相关配置文件，具体配置不再阐述请看前面的文章，配置完成之后记得执行hadoop namenode -format，否则hdfs启动会报错，启动完成后不出问题浏览器访问50070端口会看到hadoop的页面。</p>
<h2 id="四、安装配置kafka"><a href="#四、安装配置kafka" class="headerlink" title="四、安装配置kafka"></a>四、安装配置kafka</h2><p>　还是一样，先下载kafka，然后配置：</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/kafka/1.0.0/kafka_2.11-1.0.0.tgz
tar -zxvf kafka_2.11-1.0.0.tgz -C /usr/local
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> KAFKA_HOME=/usr/<span class="built_in">local</span>/kafka_2.11-1.0.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$KAFKA_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><p>　进入kafka的config目录，配置server.properties,指定log.dirs和zookeeper.connect参数；配置zookeeper.properties文件中zookeeper的dataDir，配置完成后启动kafka</p>
<pre><code>kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
</code></pre><p>　可以用jps查看有没有kafka进程，然后测试一下kafka是否能够正常收发消息，开两个终端，一个用来做producer发消息一个用来做consumer收消息，首先，先创建一个topic</p>
<pre><code>kafka-topics.sh --create --zookeeper 127.0.0.1:2181 --replication-factor 1 --partitions 1 --topic testTopic
kafka-topics.sh --describe --zookeeper localhost:2181 --topic testTopic
</code></pre><p> 如果不出一下会看到如下输出：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">Topic:</span>testTopic	<span class="string">PartitionCount:</span><span class="number">1</span>	<span class="string">ReplicationFactor:</span><span class="number">1</span>	<span class="string">Configs:</span></span><br><span class="line"><span class="string">Topic:</span> testTopic	<span class="string">Partition:</span> <span class="number">0</span>	<span class="string">Leader:</span> <span class="number">0</span>	<span class="string">Replicas:</span> <span class="number">0</span>	<span class="string">Isr:</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p> 然后在第一个终端中输入命令：<br>    kafka-console-producer.sh –broker-list localhost:9092 –topic testTopic<br> 在第二个终端中输入命令：<br>    kafka-console-consumer.sh –zookeeper 127.0.0.1:2181 –topic testTopic<br> 如果启动都正常，那么这两个终端将进入阻塞监听状态，在第一个终端中输入任何消息第二个终端都将会接收到。</p>
<h2 id="五、安装配置HBase"><a href="#五、安装配置HBase" class="headerlink" title="五、安装配置HBase"></a>五、安装配置HBase</h2><p>　下载并解压HBase：</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/hbase/1.2.6/hbase-1.2.6-bin.tar.gz
tar -zxvf hbase-1.2.6-bin.tar.gz -C /usr/local/
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HBASE_HOME=/usr/<span class="built_in">local</span>/hbase-1.2.6</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HBASE_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><p>　修改hbase下的配置文件,首先修改hbase-env.sh，主要修改JAVA_HOME以及相关参数，这里要说明一下HBASE_MANAGES_ZK这个参数，因为采用了自己的zookeeper，所以这里设置为false，否则hbase会自己启动一个zookeeper</p>
<pre><code>cd /usr/local/hbase-1.2.6/conf
vim hbase-env.sh
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/java/jdk1.8.0_144/</span><br><span class="line">HBASE_CLASSPATH=/usr/<span class="built_in">local</span>/hbase-1.2.6/conf</span><br><span class="line"><span class="built_in">export</span> HBASE_MASTER_OPTS=<span class="string">"<span class="variable">$HBASE_MASTER_OPTS</span> -XX:PermSize=256m -XX:MaxPermSize=1024m"</span></span><br><span class="line"><span class="built_in">export</span> HBASE_REGIONSERVER_OPTS=<span class="string">"<span class="variable">$HBASE_REGIONSERVER_OPTS</span> -XX:PermSize=256m -XX:MaxPermSize=1024m"</span></span><br><span class="line"><span class="built_in">export</span> HBASE_PID_DIR=/opt/data/hbase</span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>　然后修改hbase-site.xml，我们设置hbase的文件放在hdfs中，所以要设置hdfs地址，其中tsk1是我安装hadoop的机器的hostname，hbase.zookeeper.quorum参数是安装zookeeper的地址，这里的各种地址最好用机器名</p>
<pre><code>vim hbase-site.xml
</code></pre><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="params">&lt;configuration&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.rootdir<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span>hdfs:<span class="comment">//tsk1:9000/hbase&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.master<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span>tsk1:<span class="number">60000</span><span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.master.port<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span><span class="number">60000</span><span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.cluster.distributed<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span>true<span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.zookeeper.quorum<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span><span class="number">192.168</span><span class="number">.70</span><span class="number">.135</span><span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>zookeeper.znode.parent<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span>/hbase<span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.zookeeper.property.dataDir<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span><span class="meta-keyword">/opt/</span>data/zookeeper<span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">		<span class="params">&lt;name&gt;</span>hbase.master.info.bindAddress<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span>tsk1<span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line"><span class="params">&lt;/configuration&gt;</span></span><br></pre></td></tr></table></figure>
<p>　配置完成后启动hbase，输入命令：<br>    start-hbase.sh<br>　完成后查看日志没有报错的话测试一下hbase，用hbase shell进行测试：</p>
<pre><code>hbase shell
hbase(main):001:0&gt;create &apos;myTestTable&apos;,&apos;info&apos;
0 row(s) in 2.2460 seconds
=&gt; Hbase::Table - myTestTable
hbase(main):003:0&gt;list
TABLE                                                                                                                    
testTable                                                                                                                
1 row(s) in 0.1530 seconds

=&gt; [&quot;myTestTable&quot;]
</code></pre><p>　至此，hbase搭建成功，访问以下hadoop的页面，查看file system(菜单栏Utilities-&gt;Browse the file system)，这时可以看见base的相关文件已经载hadoop的文件系统中。<br><img src="/images/hbaseInHadoop.png"></p>
<h2 id="六、安装spark"><a href="#六、安装spark" class="headerlink" title="六、安装spark"></a>六、安装spark</h2><p>　下载spark并解压</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz
tar -zxvf spark-2.2.1-bin-hadoop2.7.tgz -C /usr/local
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/<span class="built_in">local</span>/spark-2.2.1-bin-hadoop2.7</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><h2 id="七、测试"><a href="#七、测试" class="headerlink" title="七、测试"></a>七、测试</h2><p>　至此，环境基本搭建完成，以上搭建的环境仅是服务器生产环境的一部分，涉及服务器信息、具体调优信息以及集群的搭建就不写在这里了，下面我们写一段代码整体测试一下从kafka生产消息到spark streaming接收到，然后处理消息并写入HBase。先写一个HBase的连接类HBaseHelper:</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> class HBaseHelper &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> HBaseHelper ME;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Configuration config;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection conn;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> HBaseAdmin admin;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> HBaseHelper getInstances() &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == ME) &#123;</span><br><span class="line">            ME = <span class="keyword">new</span> HBaseHelper();</span><br><span class="line">            config = HBaseConfiguration.create();</span><br><span class="line">            config.<span class="built_in">set</span>(<span class="string">"hbase.rootdir"</span>, <span class="string">"hdfs://tsk1:9000/hbase"</span>);</span><br><span class="line">            config.<span class="built_in">set</span>(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"tsk1"</span>);</span><br><span class="line">            config.<span class="built_in">set</span>(<span class="string">"hbase.zookeeper.property.clientPort"</span>, <span class="string">"2181"</span>);</span><br><span class="line">            config.<span class="built_in">set</span>(<span class="string">"hbase.defaults.for.version.skip"</span>, <span class="string">"true"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == conn) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                conn = ConnectionFactory.createConnection(config);</span><br><span class="line">                admin = <span class="keyword">new</span> HBaseAdmin(config);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ME;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">Table</span> getTable(<span class="keyword">String</span> tableName) &#123;</span><br><span class="line">        <span class="keyword">Table</span> table = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            table = conn.getTable(TableName.valueOf(tableName));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">            ex.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> table;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> putAdd(<span class="keyword">String</span> tableName, <span class="keyword">String</span> rowKey, <span class="keyword">String</span> cf, <span class="keyword">String</span> column, Long value) &#123;</span><br><span class="line">        <span class="keyword">Table</span> table = <span class="keyword">this</span>.getTable(tableName);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            table.incrementColumnValue(rowKey.getBytes(), cf.getBytes(), column.getBytes(), value);</span><br><span class="line">            System.out.<span class="built_in">println</span>(<span class="string">"OK！"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"> <span class="comment">//......以下省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>　再写一个测试类KafkaRecHbase用来做spark-submit提交</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.spark_test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="keyword">HashMap</span>;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.regex.Pattern;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Level;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.KafkaUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> class KafkaRecHbase &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Pattern SPACE = Pattern.compile(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="keyword">String</span>[] args) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        Logger.getLogger(<span class="string">"org"</span>).setLevel(Level.ERROR);</span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        sparkConf.setAppName(<span class="string">"kafkaRecHbase"</span>);</span><br><span class="line">        sparkConf.setMaster(<span class="string">"local[2]"</span>);</span><br><span class="line">        JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(sparkConf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line">        <span class="built_in">int</span> numThreads = Integer.parseInt(args[<span class="number">3</span>]);</span><br><span class="line">        Map&lt;<span class="keyword">String</span>, Integer&gt; topicMap = <span class="keyword">new</span> <span class="keyword">HashMap</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">String</span>[] topics = args[<span class="number">2</span>].<span class="built_in">split</span>(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">String</span> topic : topics) &#123;</span><br><span class="line">            topicMap.put(topic, numThreads);</span><br><span class="line">        &#125;</span><br><span class="line">        JavaPairReceiverInputDStream&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; kafkaStream =</span><br><span class="line">                KafkaUtils.createStream(ssc, args[<span class="number">0</span>], args[<span class="number">1</span>], topicMap);</span><br><span class="line">        JavaDStream&lt;<span class="keyword">String</span>&gt; lines = kafkaStream.<span class="built_in">map</span>(Tuple2::_2);</span><br><span class="line">        JavaDStream&lt;<span class="keyword">String</span>&gt; lineStr = lines.<span class="built_in">map</span>(<span class="built_in">line</span> -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">null</span> == <span class="built_in">line</span> || <span class="built_in">line</span>.equals(<span class="string">""</span>)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">String</span>[] strs = SPACE.<span class="built_in">split</span>(<span class="built_in">line</span>);</span><br><span class="line">            <span class="keyword">if</span> (strs.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">String</span> <span class="built_in">str</span> : strs) &#123;</span><br><span class="line">                    HBaseHelper.getInstances().putAdd(<span class="string">"myTestTable"</span>, <span class="built_in">str</span>, <span class="string">"info"</span>, <span class="string">"wordCunts"</span>, <span class="number">1</span>l);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"strs:"</span> + <span class="built_in">line</span>;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">                System.out.<span class="built_in">println</span>(<span class="built_in">line</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"报错了："</span> + ex.getMessage();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        lineStr.<span class="built_in">print</span>();</span><br><span class="line">        ssc.start();</span><br><span class="line">        System.out.<span class="built_in">println</span>(<span class="string">"spark 启动！！！"</span>);</span><br><span class="line">        ssc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>　编译提交到服务器，执行命令：</p>
<pre><code>spark-submit --jars $(echo /usr/local/hbase-1.2.6/lib/*.jar | tr &apos; &apos; &apos;,&apos;) --class com.test.spark.spark_test.KafkaRecHbase --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.1 /opt/FileTemp/streaming/spark-test-0.1.1.jar tsk1:2181 test testTopic 1
</code></pre><p>　没报错的话执行kafka的producer，输入几行数据在HBase内就能看到结果了！</p>
<h2 id="八、装一个Flume实时采集Nginx日志写入Kafka"><a href="#八、装一个Flume实时采集Nginx日志写入Kafka" class="headerlink" title="八、装一个Flume实时采集Nginx日志写入Kafka"></a>八、装一个Flume实时采集Nginx日志写入Kafka</h2><p>　Flume是一个用来日志采集的框架，安装和配置都比较简单，可以支持多个数据源和输出，具体可以参考Flume的文档，写的比较全<a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="external">传送门</a></p>
<p>　下载Flume并配置环境</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz
tar -zxvf apache-flume-1.8.0-bin.tar.gz -C /usr/local
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> FLUME_HOME=/usr/<span class="built_in">local</span>/apache-flume-1.8.0-bin/</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$FLUME_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><p>　写一个Flume的配置文件在flume的conf目录下：</p>
<pre><code>vim nginxStreamingKafka.conf
</code></pre><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">agent1.sources=r1</span><br><span class="line">agent1.channels=logger-channel</span><br><span class="line">agent1.sinks=kafka-sink</span><br><span class="line"></span><br><span class="line">agent1<span class="selector-class">.sources</span><span class="selector-class">.r1</span><span class="selector-class">.type</span>=exec</span><br><span class="line">agent1<span class="selector-class">.sources</span><span class="selector-class">.r1</span><span class="selector-class">.deserializer</span><span class="selector-class">.outputCharset</span>= UTF-<span class="number">8</span></span><br><span class="line">agent1<span class="selector-class">.sources</span><span class="selector-class">.r1</span><span class="selector-class">.command</span>=tail -F /opt/data/nginxLog/nginxLog<span class="selector-class">.log</span></span><br><span class="line"></span><br><span class="line">agent1<span class="selector-class">.channels</span><span class="selector-class">.logger-channel</span><span class="selector-class">.type</span>=memory</span><br><span class="line"></span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.type</span>=org<span class="selector-class">.apache</span><span class="selector-class">.flume</span><span class="selector-class">.sink</span><span class="selector-class">.kafka</span><span class="selector-class">.KafkaSink</span></span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.topic</span> = flumeKafka</span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.brokerList</span> = tsk1:<span class="number">9092</span></span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.requiredAcks</span> = <span class="number">1</span></span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.batchSize</span> = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">agent1<span class="selector-class">.sources</span><span class="selector-class">.r1</span><span class="selector-class">.channels</span>=logger-channel</span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.channel</span>=logger-channel</span><br></pre></td></tr></table></figure>
<p>　kafka创建一个名为flumeKafka的topic用来接收，然后启动flume：</p>
<pre><code>flume-ng agent --name agent1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf/nginxStreamingKafka.conf -Dflume.root.logger=INFO,console
</code></pre><p>　如果没有报错，Flume将开始采集opt/data/nginxLog/nginxLog.log中产生的日志并实时推送给kafka，再按照上面方法写一个spark streaming的处理类进行相应的处理就好。</p>
<p>　OK！全部搞定，然而~~~~就这样就搞定了？NO！！！这只是万里长征的第一步！呵呵！</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/images/wechat.jpg" alt="天狼武士 WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/images/alipay.jpg" alt="天狼武士 Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>本文作者：</strong>
      天狼武士
    </li>
    <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://www.tianshangkun.com/2018/01/26/基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建/" title="基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建">http://www.tianshangkun.com/2018/01/26/基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建/</a>
    </li>
    <li class="post-copyright-license">
      <strong>版权声明： </strong>
      本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/大数据/" rel="tag"># 大数据</a>
          
            <a href="/tags/Hadoop/" rel="tag"># Hadoop</a>
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
            <a href="/tags/SparkStreaming/" rel="tag"># SparkStreaming</a>
          
            <a href="/tags/kafka/" rel="tag"># kafka</a>
          
            <a href="/tags/HBase/" rel="tag"># HBase</a>
          
            <a href="/tags/流处理/" rel="tag"># 流处理</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/10/SpringBoot集成Shiro并用MongoDB做Session存储/" rel="next" title="SpringBoot集成Shiro并用MongoDB做Session存储">
                <i class="fa fa-chevron-left"></i> SpringBoot集成Shiro并用MongoDB做Session存储
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/05/15/ElasticSearch的搭建与数据统计/" rel="prev" title="ElasticSearch的搭建与数据统计">
                ElasticSearch的搭建与数据统计 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="SOHUCS"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/1.jpg"
               alt="天狼武士" />
          <p class="site-author-name" itemprop="name">天狼武士</p>
           
              <p class="site-description motion-element" itemprop="description">天狼武士的个人博客</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">13</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">27</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/1171031951" target="_blank" title="新浪微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  新浪微博
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、配置开发环境"><span class="nav-number">1.</span> <span class="nav-text">一、配置开发环境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、配置zookeeper、maven环境"><span class="nav-number">2.</span> <span class="nav-text">二、配置zookeeper、maven环境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、安装配置Hadoop"><span class="nav-number">3.</span> <span class="nav-text">三、安装配置Hadoop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、安装配置kafka"><span class="nav-number">4.</span> <span class="nav-text">四、安装配置kafka</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五、安装配置HBase"><span class="nav-number">5.</span> <span class="nav-text">五、安装配置HBase</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#六、安装spark"><span class="nav-number">6.</span> <span class="nav-text">六、安装spark</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#七、测试"><span class="nav-number">7.</span> <span class="nav-text">七、测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#八、装一个Flume实时采集Nginx日志写入Kafka"><span class="nav-number">8.</span> <span class="nav-text">八、装一个Flume实时采集Nginx日志写入Kafka</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">天狼武士</span>
</div>


<div class="powered-by">
  <a class="theme-link" href="http://www.tianshangkun.com">天狼武士</a> 专属博客
</div>

<div class="theme-info">
  苏ICP备 17021575
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  








  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  






  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("0i8uR7PzFKlmDYLJxOEMOCaD-gzGzoHsz", "RJ5fdEt3JYI1GSU9u7AXJcnV");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

</body>
</html>
