<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="田尚坤,java,javascript,jquery,互联网,架构,互联网技术" />








  <link rel="shortcut icon" type="image/x-icon" href="/wolf.ico?v=5.1.0" />






<meta name="description" content="天狼武士的个人博客">
<meta property="og:type" content="website">
<meta property="og:title" content="天狼武士的Blog">
<meta property="og:url" content="http://www.tianshangkun.com/index.html">
<meta property="og:site_name" content="天狼武士的Blog">
<meta property="og:description" content="天狼武士的个人博客">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="天狼武士的Blog">
<meta name="twitter:description" content="天狼武士的个人博客">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.tianshangkun.com/"/>





  <title> 天狼武士的Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  











  <div style="display: none;">
    <script src="//s95.cnzz.com/z_stat.php?id=1261718062&web_id=1261718062" language="JavaScript"></script>
  </div>






  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">天狼武士的Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">世界太大我想敲两行代码</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.tianshangkun.com/2018/05/15/ElasticSearch的搭建与数据统计/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="天狼武士">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天狼武士的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/05/15/ElasticSearch的搭建与数据统计/" itemprop="url">
                  ElasticSearch的搭建与数据统计
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-15T17:35:22+08:00">
                2018-05-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>　平台内的产品有一个数据分析，统计平台内某个商户某个时间段内（今天、昨天、7天内、30天内……）的各种数据分析，这种分析显然用MySql的count、sum、GroupBy之类的去查询是很不靠谱的，尤其是在数据量很大的情况下效率就不言而喻了，本来想着用HBase的MR来做，或者直接把各纬度的数据通过HADOOP的MR处理完存到HBase里面，后来与朋友聊天后被朋友严重鄙视了一顿，鄙视的内容基本是嫌弃我们的数据量太小根本用不到HBase更用不到MR，在朋友的极力蛊惑之下决定用ElasticSearch来实现以下简称ES，好吧，那我们还是从传统的搭建-采坑-填坑-再采坑-再填坑开始。</p>
<h2 id="1、下载并安装配置ElasticSearch"><a href="#1、下载并安装配置ElasticSearch" class="headerlink" title="1、下载并安装配置ElasticSearch"></a>1、下载并安装配置ElasticSearch</h2><p>　ElasticSearch的官网<a href="http://www.elastic.co/products/elasticsearch" target="_blank" rel="external">http://www.elastic.co/products/elasticsearch</a>找到需要的版本，我这里选择的是5.6.9的版本，不要问我为什么，因为最新版在我的未知领域有更多的坑！直接下载5.6.9是我目前用着最舒服的一个版本，ES依赖最低JDK1.8版本，所以环境变量一定要配置好</p>
<pre><code>wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.9.tar.gz
tar -zxvf elasticsearch-5.6.9.tar.gz -C /usr/local/
cd /usr/local/elasticsearch-5.6.9
</code></pre><p>　ES的配置文件全部在config目录下，其中elasticsearch.yml是主配置文件，进去后主要修改几个地方，其他的地方根据业务需求自行修改：</p>
<pre><code>vim config/其中elasticsearch.yml
</code></pre><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cluster.<span class="keyword">name</span>=tsk-es</span><br><span class="line">node.<span class="keyword">name</span>=tsk1</span><br><span class="line"><span class="built_in">path</span>.<span class="keyword">data</span>: /opt/<span class="keyword">data</span>/elastic/<span class="keyword">data</span></span><br><span class="line"><span class="built_in">path</span>.logs: /opt/<span class="keyword">data</span>/elastic/<span class="built_in">log</span></span><br><span class="line">network.host: <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line">http.port: <span class="number">9200</span></span><br></pre></td></tr></table></figure>
<p>　不要以为把elasticsearch.yml修改完就可以直接执行bin目录下的elasticsearch，会报一堆错误给你的！第一个错误就是告诉你不能用root用户启动ES，所以你要先创建一个用户，我这里创建一个叫elastic的用户然后记得给用户授文件夹的权限，然后su进入用户启动，但是先别急着进elastic用户，还有些东西需要改一下：</p>
<p>　1、修改/etc/security/limits.conf文件，否则会报max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]错误</p>
<pre><code>vim /etc/security/limits.conf
</code></pre><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">* </span>soft nofile 65536</span><br><span class="line"><span class="bullet">* </span>hard nofile 65536</span><br><span class="line"><span class="bullet">* </span>soft nproc 2048</span><br><span class="line"><span class="bullet">* </span>hard nproc 4096</span><br></pre></td></tr></table></figure>
<p>　2、修改/etc/sysctl.conf 文件否则会报max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]错误</p>
<pre><code>vim /etc/sysctl.conf
</code></pre><figure class="highlight fix"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">vm.max_map_count</span>=<span class="string">262144</span></span><br></pre></td></tr></table></figure>
<p>　全部修改完成之后就可以进入elastic用户启动ES了</p>
<pre><code>su elastic
bin/elasticsrarch
</code></pre><p>　如果不出意外的话你的ES应该正常运行了，这时用浏览器访问<a href="192.168.0.1:9200" target="_blank" rel="external">192.168.0.1:9200</a>你就会看到一串JSON字符串，证明你的ES已经启动成功，如果想要后台运行ES直接执行</p>
<pre><code>nohup bin/elasticsearch &gt; /opt/data/elastic/elastic.log 2&gt;&amp;1 &amp;
</code></pre><h2 id="2、ES操作初了解"><a href="#2、ES操作初了解" class="headerlink" title="2、ES操作初了解"></a>2、ES操作初了解</h2><p>　ES的操作都是通过HTTP请求进行的，不同的请求方式和参数针对不同的操作，比如创建一个索引就是PUT，删除一个索引用的是DELETE，查询如果没参数直接用GET就好，如果有参数或者是提交数据的话用POST，那么我们第一步肯定是先创建索引开始：</p>
<pre><code>PUT:http://192.168.0.1:9200/shopsinfo
{
&quot;mappings&quot;:{
    &quot;shopsOrder&quot;:{
        &quot;properties&quot;:{
            &quot;shopid&quot;:{
                &quot;type&quot;:&quot;string&quot;,
                &quot;index&quot;: &quot;not_analyzed&quot;
            },
            &quot;createdate&quot;:{
                &quot;type&quot;:&quot;string&quot;,
                &quot;index&quot;: &quot;not_analyzed&quot;
            },
            &quot;timestamp&quot;:{
                &quot;type&quot;:&quot;long&quot;    
            },
            &quot;paymentType&quot;:{
                &quot;type&quot;:&quot;string&quot;    ,
                &quot;index&quot;: &quot;not_analyzed&quot;
            },
            &quot;amount&quot;:{
                &quot;type&quot;:&quot;long&quot;
            }
        }
    }
}
}
</code></pre><p>　上面的意思是创建一个名叫shopsinfo的索引，里面有一个叫shopsOrder的mapping，里面有shopid,createdata,timestamp,paymentType,amount几个字段，以及分别对应的type</p>
<p>　插入数据比较简单，就是POST就好参数就是一个JSON</p>
<pre><code>POST:http://192.168.0.1:9200/shopsinfo/shopsOrder
{
    &quot;shopid&quot;: &quot;96119&quot;,
    &quot;createdate&quot;: &quot;20180410&quot;,
    &quot;timestamp&quot;: 1523289600000,
    &quot;paymentType&quot;: &quot;alipay&quot;,
    &quot;amount&quot;: 6917
}
</code></pre><p>　删除数据</p>
<pre><code>POST:http://192.168.0.1:9200/shopsinfo/shopsOrder/_delete_by_query
</code></pre><p>　查询<br>    GET/POST:<a href="http://192.168.0.1:9200/shopsinfo/shopsOrder/_search" target="_blank" rel="external">http://192.168.0.1:9200/shopsinfo/shopsOrder/_search</a></p>
<p>　关于ES的操作和其他不再过多赘述，官网有中文版，度娘上也一大堆，重中之重是ES的统计查询这是ES的关键</p>
<h2 id="3、重点之查询"><a href="#3、重点之查询" class="headerlink" title="3、重点之查询"></a>3、重点之查询</h2><p>　ES是属于倒排索引，查询的效率特别的高，但是ES的查询语句就很麻烦了，ES不管是查询、统计都是用POST的BODY以JSON的形式进行的，比如我要查询时间戳&gt;某个时间并且shopId为100000002和100000006的在SQL中是这样的：</p>
<pre><code>select * from shopsOrder where timestamp&gt;1523671189000 and shopid in (&quot;100000002&quot;,&quot;100000006&quot;)
</code></pre><p>　在ES中就得这么查：</p>
<pre><code>POST:http://192.168.0.1:9200/shopsinfo/shopsOrder/_search
{
    &quot;size&quot;:20,
    &quot;query&quot;:{
        &quot;bool&quot;:{
            &quot;must&quot;:[
                {
                    &quot;range&quot;:{
                        &quot;timestamp&quot;:{
                            &quot;gte&quot;:1523671189000
                        }

                    }
                },
                {
                    &quot;terms&quot;:{
                        &quot;shopid&quot;:[&quot;100000002&quot;,&quot;100000006&quot;]
                    }
                }
            ]
        }
    }
}
</code></pre><p>　这里面我传递了size参数，如果不传，ES默认给你返回10条数据，查询结果ES也会给你返回JSON，其中hits字段中会有total就是你查询的结果的总数hits会返回给你结果内容。</p>
<p>　以上是简单的查询，统计的话ES是以aggs作为参数，全称应该叫做Aggregation,比如接着刚才的查询我想计算出结果的amount总额是多少就是类似SQL中的</p>
<pre><code>select sum(amount)query_amount from shopsOrder where timestamp&gt;1523671189000 and shopid in (&quot;100000002&quot;,&quot;100000006&quot;)
</code></pre><p>　在ES中就得这么查</p>
<pre><code>{
    &quot;aggs&quot;:{
        &quot;query_amount&quot;:{
            &quot;sum&quot;:{
                &quot;field&quot;:&quot;amount&quot;
            }
        }
    },
    &quot;query&quot;:{
        &quot;bool&quot;:{
            &quot;must&quot;:[
                {
                    &quot;range&quot;:{
                        &quot;timestamp&quot;:{
                            &quot;gte&quot;:1523671189000
                        }

                    }
                },
                {
                    &quot;terms&quot;:{
                        &quot;shopid&quot;:[&quot;100000002&quot;,&quot;100000006&quot;]
                    }
                }
            ]
        }
    }
}
</code></pre><p>　统计的结果在返回值的aggregations参数里的query_amount下类似这样的：</p>
<pre><code>......
&quot;aggregations&quot;: {
    &quot;query_amount&quot;: {
        &quot;value&quot;: 684854
    }
}
......
</code></pre><p>　接下来再复杂一点点，按天分组进行统计查询SQL中的提现是这样的：</p>
<pre><code>select createdate,sum(amount)query_amount from shopsOrder where timestamp&gt;1523671189000 and shopid in (&quot;100000002&quot;,&quot;100000006&quot;)
group by createdate order by createdate
</code></pre><p>　在ES中是这样的：</p>
<pre><code>{
    &quot;size&quot;:0,
    &quot;aggs&quot;:{
        &quot;orderDate&quot;:{
            &quot;terms&quot;:{
                &quot;field&quot;:&quot;createdate&quot;,
                &quot;order&quot;:{
                    &quot;_term&quot;:&quot;asc&quot;
                }
            },
            &quot;aggs&quot;:{
                &quot;query_amount&quot;:{
                    &quot;sum&quot;:{
                        &quot;field&quot;:&quot;amount&quot;
                    }
                }
            }
        }
    },
    &quot;query&quot;:{
        &quot;bool&quot;:{
            &quot;must&quot;:[
                {
                    &quot;range&quot;:{
                        &quot;timestamp&quot;:{
                            &quot;gte&quot;:1523671189000
                        }

                    }
                },
                {
                    &quot;terms&quot;:{
                        &quot;shopid&quot;:[&quot;100000002&quot;,&quot;100000006&quot;]
                    }
                }
            ]
        }
    }
}
</code></pre><p>　查询结果为</p>
<pre><code>......
&quot;aggregations&quot;: {
    &quot;orderDate&quot;: {
        &quot;doc_count_error_upper_bound&quot;: 0,
        &quot;sum_other_doc_count&quot;: 99,
        &quot;buckets&quot;: [
            ......
            {
                &quot;key&quot;: &quot;20180415&quot;,
                &quot;doc_count&quot;: 8,
                &quot;query_amount&quot;: {
                    &quot;value&quot;: 31632
                }
            },
            {
                &quot;key&quot;: &quot;20180417&quot;,
                &quot;doc_count&quot;: 3,
                &quot;query_amount&quot;: {
                    &quot;value&quot;: 21401
                }
            },
            {
                &quot;key&quot;: &quot;20180418&quot;,
                &quot;doc_count&quot;: 2,
                &quot;query_amount&quot;: {
                    &quot;value&quot;: 2333
                }
            }
            ......
        ]
    }
}
</code></pre><p>　buckets中就是查询的结果，key为按我createdate分组后的值，doc_count类似count,query_amount为sum后的值。至于我的参数里面有一个size:0是因为我不需要具体的记录就是hits，所以这里传0</p>
<p>　最后我们来个更复杂的1、统计所有的总额；2、先按paymentType支付方式分组统计amount总额，并且每个支付方式中再按天分组统计每天的amount总额</p>
<pre><code>    {
    &quot;size&quot;:0,
    &quot;aggs&quot;:{
        &quot;amount&quot;:{
            &quot;sum&quot;:{
                &quot;field&quot;:&quot;amount&quot;
            }
        },
        &quot;paymenttype&quot;:{
            &quot;terms&quot;:{
                &quot;field&quot;:&quot;paymentType&quot;
            },
            &quot;aggs&quot;:{
                &quot;query_amount&quot;:{
                    &quot;sum&quot;:{
                        &quot;field&quot;:&quot;amount&quot;
                    }
                },
                &quot;payment_date&quot;:{
                    &quot;terms&quot;:{
                        &quot;field&quot;:&quot;createdate&quot;
                    },
                    &quot;aggs&quot;:{
                        &quot;query_amount&quot;:{
                            &quot;sum&quot;:{
                                &quot;field&quot;:&quot;amount&quot;
                            }
                        }
                    }
                }
            }
        }
    },
    &quot;query&quot;:{
        &quot;bool&quot;:{
            &quot;must&quot;:[
                {
                    &quot;range&quot;:{
                        &quot;timestamp&quot;:{
                            &quot;gte&quot;:1523671189000
                        }

                    }
                },
                {
                    &quot;terms&quot;:{
                        &quot;shopid&quot;:[&quot;100000002&quot;,&quot;100000006&quot;]
                    }
                }
            ]
        }
    }
}
</code></pre><p>　查询结果为：</p>
<pre><code>......
&quot;amount&quot;: {
    &quot;value&quot;: 684854
},
&quot;paymenttype&quot;:{
     ......
    &quot;buckets&quot;: [
        {
            &quot;key&quot;: &quot;wechatpay&quot;,
            &quot;doc_count&quot;: 73,
            &quot;amount&quot;: {
                &quot;value&quot;: 351142
            },
            &quot;payment_date&quot;: {
                &quot;doc_count_error_upper_bound&quot;: 0,
                &quot;sum_other_doc_count&quot;: 25,
                &quot;buckets&quot;: [
                    ......
                    {
                        &quot;key&quot;: &quot;20180415&quot;,
                        &quot;doc_count&quot;: 6,
                        &quot;amount&quot;: {
                            &quot;value&quot;: 29032
                        }
                    },
                    {
                        &quot;key&quot;: &quot;20180425&quot;,
                        &quot;doc_count&quot;: 6,
                        &quot;amount&quot;: {
                            &quot;value&quot;: 21592
                        }
                    }
                    ......
                ]
            }
        },
        {
            &quot;key&quot;: &quot;alipay&quot;,
            &quot;doc_count&quot;: 67,
            &quot;amount&quot;: {
                &quot;value&quot;: 333712
            },
            &quot;payment_date&quot;: {
                &quot;doc_count_error_upper_bound&quot;: 0,
                &quot;sum_other_doc_count&quot;: 23,
                &quot;buckets&quot;: [
                    ......
                    {
                        &quot;key&quot;: &quot;20180506&quot;,
                        &quot;doc_count&quot;: 8,
                        &quot;amount&quot;: {
                            &quot;value&quot;: 38280
                        }
                    },
                    {
                        &quot;key&quot;: &quot;20180426&quot;,
                        &quot;doc_count&quot;: 6,
                        &quot;amount&quot;: {
                            &quot;value&quot;: 41052
                        }
                    }
                    ......
                ]
            }
        }
    ]
}
</code></pre><h2 id="4、JAVA操作ES"><a href="#4、JAVA操作ES" class="headerlink" title="4、JAVA操作ES"></a>4、JAVA操作ES</h2><p>　根据自己下载的ES版本下载对应版本的JAR包,我安装的是5.6.9所以我的JAR包版本也应该是5.6.9</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
    &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
    &lt;version&gt;5.6.9&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;
    &lt;artifactId&gt;transport&lt;/artifactId&gt;
    &lt;version&gt;5.6.9&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><p>　创建一个helper操作ES，由于我的ES项目是基于SpringBoot的所以我的helper决定交由spring去管理，其实写一个单例也是可以的，先创建client连接<br></p>
<pre><code>import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.transport.client.PreBuiltTransportClient;
import org.elasticsearch.common.transport.InetSocketTransportAddress;
import java.net.InetAddress;

Settings settings = Settings.builder().put(&quot;cluster.name&quot;, &quot;tsk-es&quot;).put(&quot;client.transport.sniff&quot;, true)
                    .build();
TransportClient client = new PreBuiltTransportClient(settings)
                    .addTransportAddresses(new InetSocketTransportAddress(InetAddress.getByName(HOST), PORT));
</code></pre><p>　插入数据比较简单你可以直接插入JSON字符串，也可以传递JAVA BEAN</p>
<pre><code>import org.elasticsearch.action.index.IndexResponse;
import org.elasticsearch.common.xcontent.XContentType;

IndexResponse response = client.prepareIndex(index, mapping).setSource(jsonStr, XContentType.JSON)
                .get();
</code></pre><p>　查询就比较麻烦了，已上面最后一个查询先按paymentType支付方式分组统计amount总额，并且每个支付方式中再按天分组统计每天的amount总额为例：</p>
<pre><code>public void getAmountData(Long startTimestamp, String... shopIds) {
    // 先初始化一个SearchRequestBuilder，指向shopsInfo/shopsOrder
    SearchRequestBuilder sbuilder = client.prepareSearch(&quot;shopsinfo&quot;).setTypes(&quot;shopsOrder&quot;);
    // 条件一
    TermsQueryBuilder mpq = QueryBuilders.termsQuery(&quot;shopid&quot;, shopIds);
    // 条件二
    RangeQueryBuilder mpq2 = QueryBuilders.rangeQuery(&quot;timestamp&quot;).gte(startTimestamp);
    // 初始化QueryBuilder
    QueryBuilder queryBuilder = QueryBuilders.boolQuery().must(mpq).must(mpq2);
    // 将QueryBuilder放入SearchRequestBuilder
    sbuilder.setQuery(queryBuilder);
    sbuilder.setSize(0);

    // sum，这里创建一个实例全部用这个实例就行
    SumAggregationBuilder salaryAgg = AggregationBuilders.sum(&quot;query_amount&quot;).field(&quot;amount&quot;);

    TermsAggregationBuilder paymentAgg = AggregationBuilders.terms(&quot;paymentType&quot;).field(&quot;paymentType&quot;);
    paymentAgg.size(100);
    paymentAgg.subAggregation(salaryAgg);
    TermsAggregationBuilder groupDateAff = AggregationBuilders.terms(&quot;payment_date&quot;).field(&quot;createdate&quot;)
            .order(Order.term(true));
    groupDateAff.size(100);
    groupDateAff.subAggregation(salaryAgg);
    paymentAgg.subAggregation(groupDateAff);
    // 将统计查询放入SearchRequestBuilder
    sbuilder.addAggregation(salaryAgg).addAggregation(paymentAgg);
    SearchResponse response = sbuilder.execute().actionGet();
    Map&lt;String, Aggregation&gt; aggMap = response.getAggregations().asMap();
    // 获取全部的总额
    InternalSum shopGroupAllAmount = (InternalSum) aggMap.get(&quot;amount&quot;);
    Double amount = shopGroupAllAmount.getValue();
    ......
}
</code></pre><p>　SearchResponse中就已经可以获取到全部的信息，至于后续怎么解析数据那就看具体业务需求以及每个人的习惯。ES的各种操作说简单也简单说复杂也复杂，按照朋友的话说就是用熟了自然就简单，确实也是这样，不管用啥都得深入了解一下，要不然自己就是个坑！比如我在做统计查询的时候返回的结果总是比应该的结果要少很多，而少的数量总是出现在sum_other_doc_count这个字段里，研究了半天才发现原来统计的结果也需要传递size参数，否则一样默认10条！</p>
<p>　最后，还是要感谢一下我那朋友的，虽然他对我想构建各种大数据平台的事情嗤之以鼻（因为我们数据量确实不大），但依然输出了一下他所能想到的最优解。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.tianshangkun.com/2018/01/26/基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="天狼武士">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天狼武士的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/01/26/基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建/" itemprop="url">
                  基于Hadoop生态SparkStreaming的大数据实时流处理平台的搭建
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-26T16:53:54+08:00">
                2018-01-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>　随着公司业务发展，对大数据的获取和实时处理的要求就会越来越高，日志处理、用户行为分析、场景业务分析等等，传统的写日志方式根本满足不了业务的实时处理需求，所以本人准备开始着手改造原系统中的数据处理方式，重新搭建一个实时流处理平台，主要是基于Hadoop生态，利用Kafka作为中转，SparkStreaming框架实时获取数据并清洗，将结果多维度的存储进HBase数据库。<br>　整个平台大致的框架如下：<br>　<img src="/images/streaming.png"></p>
<p>　操作系统：Centos7<br>　用到的框架：<br>　1. <a href="http://flume.apache.org/" target="_blank" rel="external">Flume1.8.0</a><br>　2. <a href="http://hadoop.apache.org/" target="_blank" rel="external">Hadoop2.9.0</a><br>　3. <a href="http://kafka.apache.org/" target="_blank" rel="external">kafka2.11-1.0.0</a><br>　4. <a href="spark.apache.org">Spark2.2.1</a><br>　5. <a href="hbase.apache.org">HBase1.2.6</a><br>　6. <a href="http://zookeeper.apache.org/" target="_blank" rel="external">ZooKeeper3.4.11</a><br>　7. <a href="http://maven.apache.org/" target="_blank" rel="external">maven3.5.2</a><br>　整体的开发环境是基于JDK1.8以上以及Scala，所以得提前把java和Scala的环境给准备好，接下来就开始着手搭建基础平台：</p>
<h2 id="一、配置开发环境"><a href="#一、配置开发环境" class="headerlink" title="一、配置开发环境"></a>一、配置开发环境</h2><p>　下载并解压JDK1.8,、下载并解压Scala，配置profile文件：</p>
<pre><code>vim /etc/profile
</code></pre><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=<span class="regexp">/usr/java</span><span class="regexp">/jdk1.8.0_144</span><br><span class="line">export PATH=$JAVA_HOME/bin</span>:$PATH</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/<span class="class"><span class="keyword">lib</span>/<span class="title">dt</span>.<span class="title">jar</span>:$<span class="title">JAVA_HOME</span>/<span class="title">lib</span>/<span class="title">tools</span>.<span class="title">jar</span></span></span><br><span class="line">export SCALA_HOME=<span class="regexp">/usr/local</span><span class="regexp">/scala-2.11.12</span><br><span class="line">export PATH=$PATH:$SCALA_HOME/bin</span></span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><h2 id="二、配置zookeeper、maven环境"><a href="#二、配置zookeeper、maven环境" class="headerlink" title="二、配置zookeeper、maven环境"></a>二、配置zookeeper、maven环境</h2><p>　下载并解压zookeeper以及maven并配置profile文件</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz
tar -zxvf apache-maven-3.5.2-bin.tar.gz -C /usr/local
wget http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz
tar -zxvf zookeeper-3.4.11.tar.gz -C /usr/local
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> MAVEN_HOME=/usr/<span class="built_in">local</span>/apache-maven-3.5.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$MAVEN_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><p>　zookeeper的配置文件配置一下：</p>
<pre><code>cp /usr/local/zookeeper-3.4.11/conf/zoo_sample.cfg /usr/local/zookeeper-3.4.11/conf/zoo.cfg
</code></pre><p>　然后配置一下zoo.cfg里面的相关配置，指定一下dataDir目录等等<br>　启动zookeeper:</p>
<pre><code>/usr/local/zookeeper-3.4.11/bin/zkServer.sh start
</code></pre><p>　如果不报错，jps看一下是否启动成功</p>
<h2 id="三、安装配置Hadoop"><a href="#三、安装配置Hadoop" class="headerlink" title="三、安装配置Hadoop"></a>三、安装配置Hadoop</h2><p>　Hadoop的安装配置在之前文章中有说过（<a href="/2017/06/13/Centos下docker搭建Hadoop集群/">传送门</a>），为了下面的步骤方便理解，这里只做一个单机版的简单配置说明：<br>　下载hadoop解压并配置环境：</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.9.0/hadoop-2.9.0.tar.gz
tar -zxvf hadoop-2.9.0.tar.gz -C /usr/local
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop-2.9.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><p>　配置hadoop 进入/usr/local/hadoop-2.9.0/etc/hadoop目录</p>
<pre><code>cd /usr/local/hadoop-2.9.0/etc/hadoop
</code></pre><p>　首先配置hadoop-env.sh、yarn-env.sh，修改JAVA_HOME到指定的JDK安装目录/usr/local/java/jdk1.8.0_144<br>　创建hadoop的工作目录</p>
<pre><code>mkdir /opt/data/hadoop
</code></pre><p>　编辑core-site.xml、hdfs-site.xml、yarn-site.xml等相关配置文件，具体配置不再阐述请看前面的文章，配置完成之后记得执行hadoop namenode -format，否则hdfs启动会报错，启动完成后不出问题浏览器访问50070端口会看到hadoop的页面。</p>
<h2 id="四、安装配置kafka"><a href="#四、安装配置kafka" class="headerlink" title="四、安装配置kafka"></a>四、安装配置kafka</h2><p>　还是一样，先下载kafka，然后配置：</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/kafka/1.0.0/kafka_2.11-1.0.0.tgz
tar -zxvf kafka_2.11-1.0.0.tgz -C /usr/local
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> KAFKA_HOME=/usr/<span class="built_in">local</span>/kafka_2.11-1.0.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$KAFKA_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><p>　进入kafka的config目录，配置server.properties,指定log.dirs和zookeeper.connect参数；配置zookeeper.properties文件中zookeeper的dataDir，配置完成后启动kafka</p>
<pre><code>kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
</code></pre><p>　可以用jps查看有没有kafka进程，然后测试一下kafka是否能够正常收发消息，开两个终端，一个用来做producer发消息一个用来做consumer收消息，首先，先创建一个topic</p>
<pre><code>kafka-topics.sh --create --zookeeper 127.0.0.1:2181 --replication-factor 1 --partitions 1 --topic testTopic
kafka-topics.sh --describe --zookeeper localhost:2181 --topic testTopic
</code></pre><p> 如果不出一下会看到如下输出：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">Topic:</span>testTopic	<span class="string">PartitionCount:</span><span class="number">1</span>	<span class="string">ReplicationFactor:</span><span class="number">1</span>	<span class="string">Configs:</span></span><br><span class="line"><span class="string">Topic:</span> testTopic	<span class="string">Partition:</span> <span class="number">0</span>	<span class="string">Leader:</span> <span class="number">0</span>	<span class="string">Replicas:</span> <span class="number">0</span>	<span class="string">Isr:</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p> 然后在第一个终端中输入命令：<br>    kafka-console-producer.sh –broker-list localhost:9092 –topic testTopic<br> 在第二个终端中输入命令：<br>    kafka-console-consumer.sh –zookeeper 127.0.0.1:2181 –topic testTopic<br> 如果启动都正常，那么这两个终端将进入阻塞监听状态，在第一个终端中输入任何消息第二个终端都将会接收到。</p>
<h2 id="五、安装配置HBase"><a href="#五、安装配置HBase" class="headerlink" title="五、安装配置HBase"></a>五、安装配置HBase</h2><p>　下载并解压HBase：</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/hbase/1.2.6/hbase-1.2.6-bin.tar.gz
tar -zxvf hbase-1.2.6-bin.tar.gz -C /usr/local/
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HBASE_HOME=/usr/<span class="built_in">local</span>/hbase-1.2.6</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HBASE_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><p>　修改hbase下的配置文件,首先修改hbase-env.sh，主要修改JAVA_HOME以及相关参数，这里要说明一下HBASE_MANAGES_ZK这个参数，因为采用了自己的zookeeper，所以这里设置为false，否则hbase会自己启动一个zookeeper</p>
<pre><code>cd /usr/local/hbase-1.2.6/conf
vim hbase-env.sh
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/java/jdk1.8.0_144/</span><br><span class="line">HBASE_CLASSPATH=/usr/<span class="built_in">local</span>/hbase-1.2.6/conf</span><br><span class="line"><span class="built_in">export</span> HBASE_MASTER_OPTS=<span class="string">"<span class="variable">$HBASE_MASTER_OPTS</span> -XX:PermSize=256m -XX:MaxPermSize=1024m"</span></span><br><span class="line"><span class="built_in">export</span> HBASE_REGIONSERVER_OPTS=<span class="string">"<span class="variable">$HBASE_REGIONSERVER_OPTS</span> -XX:PermSize=256m -XX:MaxPermSize=1024m"</span></span><br><span class="line"><span class="built_in">export</span> HBASE_PID_DIR=/opt/data/hbase</span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>　然后修改hbase-site.xml，我们设置hbase的文件放在hdfs中，所以要设置hdfs地址，其中tsk1是我安装hadoop的机器的hostname，hbase.zookeeper.quorum参数是安装zookeeper的地址，这里的各种地址最好用机器名</p>
<pre><code>vim hbase-site.xml
</code></pre><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="params">&lt;configuration&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.rootdir<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span>hdfs:<span class="comment">//tsk1:9000/hbase&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.master<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span>tsk1:<span class="number">60000</span><span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.master.port<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span><span class="number">60000</span><span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.cluster.distributed<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span>true<span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.zookeeper.quorum<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span><span class="number">192.168</span><span class="number">.70</span><span class="number">.135</span><span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>zookeeper.znode.parent<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span>/hbase<span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">        <span class="params">&lt;name&gt;</span>hbase.zookeeper.property.dataDir<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span><span class="meta-keyword">/opt/</span>data/zookeeper<span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line">    <span class="params">&lt;property&gt;</span></span><br><span class="line">		<span class="params">&lt;name&gt;</span>hbase.master.info.bindAddress<span class="params">&lt;/name&gt;</span></span><br><span class="line">        <span class="params">&lt;value&gt;</span>tsk1<span class="params">&lt;/value&gt;</span></span><br><span class="line">    <span class="params">&lt;/property&gt;</span></span><br><span class="line"><span class="params">&lt;/configuration&gt;</span></span><br></pre></td></tr></table></figure>
<p>　配置完成后启动hbase，输入命令：<br>    start-hbase.sh<br>　完成后查看日志没有报错的话测试一下hbase，用hbase shell进行测试：</p>
<pre><code>hbase shell
hbase(main):001:0&gt;create &apos;myTestTable&apos;,&apos;info&apos;
0 row(s) in 2.2460 seconds
=&gt; Hbase::Table - myTestTable
hbase(main):003:0&gt;list
TABLE                                                                                                                    
testTable                                                                                                                
1 row(s) in 0.1530 seconds

=&gt; [&quot;myTestTable&quot;]
</code></pre><p>　至此，hbase搭建成功，访问以下hadoop的页面，查看file system(菜单栏Utilities-&gt;Browse the file system)，这时可以看见base的相关文件已经载hadoop的文件系统中。<br><img src="/images/hbaseInHadoop.png"></p>
<h2 id="六、安装spark"><a href="#六、安装spark" class="headerlink" title="六、安装spark"></a>六、安装spark</h2><p>　下载spark并解压</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz
tar -zxvf spark-2.2.1-bin-hadoop2.7.tgz -C /usr/local
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/<span class="built_in">local</span>/spark-2.2.1-bin-hadoop2.7</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><h2 id="七、测试"><a href="#七、测试" class="headerlink" title="七、测试"></a>七、测试</h2><p>　至此，环境基本搭建完成，以上搭建的环境仅是服务器生产环境的一部分，涉及服务器信息、具体调优信息以及集群的搭建就不写在这里了，下面我们写一段代码整体测试一下从kafka生产消息到spark streaming接收到，然后处理消息并写入HBase。先写一个HBase的连接类HBaseHelper:</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> class HBaseHelper &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> HBaseHelper ME;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Configuration config;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection conn;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> HBaseAdmin admin;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> HBaseHelper getInstances() &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == ME) &#123;</span><br><span class="line">            ME = <span class="keyword">new</span> HBaseHelper();</span><br><span class="line">            config = HBaseConfiguration.create();</span><br><span class="line">            config.<span class="built_in">set</span>(<span class="string">"hbase.rootdir"</span>, <span class="string">"hdfs://tsk1:9000/hbase"</span>);</span><br><span class="line">            config.<span class="built_in">set</span>(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"tsk1"</span>);</span><br><span class="line">            config.<span class="built_in">set</span>(<span class="string">"hbase.zookeeper.property.clientPort"</span>, <span class="string">"2181"</span>);</span><br><span class="line">            config.<span class="built_in">set</span>(<span class="string">"hbase.defaults.for.version.skip"</span>, <span class="string">"true"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == conn) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                conn = ConnectionFactory.createConnection(config);</span><br><span class="line">                admin = <span class="keyword">new</span> HBaseAdmin(config);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ME;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">Table</span> getTable(<span class="keyword">String</span> tableName) &#123;</span><br><span class="line">        <span class="keyword">Table</span> table = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            table = conn.getTable(TableName.valueOf(tableName));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">            ex.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> table;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> putAdd(<span class="keyword">String</span> tableName, <span class="keyword">String</span> rowKey, <span class="keyword">String</span> cf, <span class="keyword">String</span> column, Long value) &#123;</span><br><span class="line">        <span class="keyword">Table</span> table = <span class="keyword">this</span>.getTable(tableName);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            table.incrementColumnValue(rowKey.getBytes(), cf.getBytes(), column.getBytes(), value);</span><br><span class="line">            System.out.<span class="built_in">println</span>(<span class="string">"OK！"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"> <span class="comment">//......以下省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>　再写一个测试类KafkaRecHbase用来做spark-submit提交</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.spark_test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="keyword">HashMap</span>;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.regex.Pattern;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Level;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.KafkaUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> class KafkaRecHbase &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Pattern SPACE = Pattern.compile(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="keyword">String</span>[] args) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        Logger.getLogger(<span class="string">"org"</span>).setLevel(Level.ERROR);</span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        sparkConf.setAppName(<span class="string">"kafkaRecHbase"</span>);</span><br><span class="line">        sparkConf.setMaster(<span class="string">"local[2]"</span>);</span><br><span class="line">        JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(sparkConf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line">        <span class="built_in">int</span> numThreads = Integer.parseInt(args[<span class="number">3</span>]);</span><br><span class="line">        Map&lt;<span class="keyword">String</span>, Integer&gt; topicMap = <span class="keyword">new</span> <span class="keyword">HashMap</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">String</span>[] topics = args[<span class="number">2</span>].<span class="built_in">split</span>(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">String</span> topic : topics) &#123;</span><br><span class="line">            topicMap.put(topic, numThreads);</span><br><span class="line">        &#125;</span><br><span class="line">        JavaPairReceiverInputDStream&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; kafkaStream =</span><br><span class="line">                KafkaUtils.createStream(ssc, args[<span class="number">0</span>], args[<span class="number">1</span>], topicMap);</span><br><span class="line">        JavaDStream&lt;<span class="keyword">String</span>&gt; lines = kafkaStream.<span class="built_in">map</span>(Tuple2::_2);</span><br><span class="line">        JavaDStream&lt;<span class="keyword">String</span>&gt; lineStr = lines.<span class="built_in">map</span>(<span class="built_in">line</span> -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">null</span> == <span class="built_in">line</span> || <span class="built_in">line</span>.equals(<span class="string">""</span>)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">String</span>[] strs = SPACE.<span class="built_in">split</span>(<span class="built_in">line</span>);</span><br><span class="line">            <span class="keyword">if</span> (strs.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">String</span> <span class="built_in">str</span> : strs) &#123;</span><br><span class="line">                    HBaseHelper.getInstances().putAdd(<span class="string">"myTestTable"</span>, <span class="built_in">str</span>, <span class="string">"info"</span>, <span class="string">"wordCunts"</span>, <span class="number">1</span>l);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"strs:"</span> + <span class="built_in">line</span>;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">                System.out.<span class="built_in">println</span>(<span class="built_in">line</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"报错了："</span> + ex.getMessage();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        lineStr.<span class="built_in">print</span>();</span><br><span class="line">        ssc.start();</span><br><span class="line">        System.out.<span class="built_in">println</span>(<span class="string">"spark 启动！！！"</span>);</span><br><span class="line">        ssc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>　编译提交到服务器，执行命令：</p>
<pre><code>spark-submit --jars $(echo /usr/local/hbase-1.2.6/lib/*.jar | tr &apos; &apos; &apos;,&apos;) --class com.test.spark.spark_test.KafkaRecHbase --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.1 /opt/FileTemp/streaming/spark-test-0.1.1.jar tsk1:2181 test testTopic 1
</code></pre><p>　没报错的话执行kafka的producer，输入几行数据在HBase内就能看到结果了！</p>
<h2 id="八、装一个Flume实时采集Nginx日志写入Kafka"><a href="#八、装一个Flume实时采集Nginx日志写入Kafka" class="headerlink" title="八、装一个Flume实时采集Nginx日志写入Kafka"></a>八、装一个Flume实时采集Nginx日志写入Kafka</h2><p>　Flume是一个用来日志采集的框架，安装和配置都比较简单，可以支持多个数据源和输出，具体可以参考Flume的文档，写的比较全<a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="external">传送门</a></p>
<p>　下载Flume并配置环境</p>
<pre><code>wget http://mirrors.hust.edu.cn/apache/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz
tar -zxvf apache-flume-1.8.0-bin.tar.gz -C /usr/local
vim /etc/profile
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> FLUME_HOME=/usr/<span class="built_in">local</span>/apache-flume-1.8.0-bin/</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$FLUME_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<pre><code>source /etc/profile
</code></pre><p>　写一个Flume的配置文件在flume的conf目录下：</p>
<pre><code>vim nginxStreamingKafka.conf
</code></pre><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">agent1.sources=r1</span><br><span class="line">agent1.channels=logger-channel</span><br><span class="line">agent1.sinks=kafka-sink</span><br><span class="line"></span><br><span class="line">agent1<span class="selector-class">.sources</span><span class="selector-class">.r1</span><span class="selector-class">.type</span>=exec</span><br><span class="line">agent1<span class="selector-class">.sources</span><span class="selector-class">.r1</span><span class="selector-class">.deserializer</span><span class="selector-class">.outputCharset</span>= UTF-<span class="number">8</span></span><br><span class="line">agent1<span class="selector-class">.sources</span><span class="selector-class">.r1</span><span class="selector-class">.command</span>=tail -F /opt/data/nginxLog/nginxLog<span class="selector-class">.log</span></span><br><span class="line"></span><br><span class="line">agent1<span class="selector-class">.channels</span><span class="selector-class">.logger-channel</span><span class="selector-class">.type</span>=memory</span><br><span class="line"></span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.type</span>=org<span class="selector-class">.apache</span><span class="selector-class">.flume</span><span class="selector-class">.sink</span><span class="selector-class">.kafka</span><span class="selector-class">.KafkaSink</span></span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.topic</span> = flumeKafka</span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.brokerList</span> = tsk1:<span class="number">9092</span></span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.requiredAcks</span> = <span class="number">1</span></span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.batchSize</span> = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">agent1<span class="selector-class">.sources</span><span class="selector-class">.r1</span><span class="selector-class">.channels</span>=logger-channel</span><br><span class="line">agent1<span class="selector-class">.sinks</span><span class="selector-class">.kafka-sink</span><span class="selector-class">.channel</span>=logger-channel</span><br></pre></td></tr></table></figure>
<p>　kafka创建一个名为flumeKafka的topic用来接收，然后启动flume：</p>
<pre><code>flume-ng agent --name agent1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf/nginxStreamingKafka.conf -Dflume.root.logger=INFO,console
</code></pre><p>　如果没有报错，Flume将开始采集opt/data/nginxLog/nginxLog.log中产生的日志并实时推送给kafka，再按照上面方法写一个spark streaming的处理类进行相应的处理就好。</p>
<p>　OK！全部搞定，然而~~~~就这样就搞定了？NO！！！这只是万里长征的第一步！呵呵！</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.tianshangkun.com/2017/11/10/SpringBoot集成Shiro并用MongoDB做Session存储/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="天狼武士">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天狼武士的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/11/10/SpringBoot集成Shiro并用MongoDB做Session存储/" itemprop="url">
                  SpringBoot集成Shiro并用MongoDB做Session存储
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-10T14:04:12+08:00">
                2017-11-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>　之前项目鉴权一直使用的Shiro，那是在Spring MVC里面使用的比较多，而且都是用XML来配置，用Shiro来做权限控制相对比较简单而且成熟，而且我一直都把Shiro的session放在mongodb中，这个比较符合mongodb的设计初衷，而且在分布式项目中mongodb也作为一个中间层，用来很好很方便解决分布式环境下的session同步的问题</p>
<p>　自从SpringBoot问世之后我的项目基本上能用SpringBoot的就会用SpringBoot，用MAVEN做统一集中管理也很方便，虽然SpringBoot也提供了一套权限安全框架Spring Security，但是相对来说还是不是太好用，所以还是用Shiro来的方便一点，SpringBoot集成Shiro要比Spring MVC要简单的多，至少没有一堆XML配置，看起来更清爽，那么接下来我们就开始集成。</p>
<p>　第一步必然是在MAVEN中先添加Shiro和mongo的依赖，我用的Shiro版本是</p>
<pre><code>&lt;shiro.version&gt;1.2.3&lt;/shiro.version&gt;
</code></pre><p>　添加依赖：</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt;
    &lt;artifactId&gt;shiro-core&lt;/artifactId&gt;
    &lt;version&gt;${shiro.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt;
    &lt;artifactId&gt;shiro-web&lt;/artifactId&gt;
    &lt;version&gt;${shiro.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt;
    &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt;
    &lt;version&gt;${shiro.version}&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.mongodb&lt;/groupId&gt;
      &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt;
      &lt;version&gt;3.0.0&lt;/version&gt;
&lt;/dependency&gt;
  &lt;dependency&gt;
  &lt;groupId&gt;org.springframework.data&lt;/groupId&gt;
  &lt;artifactId&gt;spring-data-mongodb&lt;/artifactId&gt;
  &lt;version&gt;1.7.0.RELEASE&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><p>　然后在application.xml或yml中配置mongodb</p>
<pre><code>spring.data.mongodb.host=127.0.0.1
spring.data.mongodb.port=27017
spring.data.mongodb.database=SHIRO_INFO
</code></pre><p>　配置完成之后我们开始正式写Shiro认证的代码，先自定义一个鉴权realm，继承自AuthorizingRealm</p>
<pre><code>public class ShiroDbRealm extends AuthorizingRealm {

  /**
   * 用户信息操作
   */
  private SystemUserService systemUserService;

  public ShiroDbRealm() {}

  public ShiroDbRealm(SystemUserService systemUserService) {
    this.systemUserService = systemUserService;
  }

  /**
   * 授权信息
   */
  @Override
  protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) {
    SimpleAuthorizationInfo info = (SimpleAuthorizationInfo) ShiroKit.getShiroSessionAttr(&quot;perms&quot;);
    if (null != info &amp;&amp; !CollectionUtils.isEmpty(info.getRoles())
        &amp;&amp; !CollectionUtils.isEmpty(info.getStringPermissions())) {
      return info;

    }
    return null;

  }

  /**
   * 认证信息
   */
  protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authcToken)
      throws AuthenticationException {
    UsernamePasswordToken token = (UsernamePasswordToken) authcToken;
    String userName = token.getUsername();
    if (userName != null &amp;&amp; !&quot;&quot;.equals(userName)) {
      SystemUser key = new SystemUser();
      key.setLoginName(token.getUsername());
      key.setPassword(String.valueOf(token.getPassword()));
      SystemUser user = systemUserService.login(key);

      if (user != null) {
        Subject userTemp = SecurityUtils.getSubject();
        userTemp.getSession().setAttribute(&quot;userId&quot;, user.getId());
        userTemp.getSession().setAttribute(&quot;userName&quot;, user.getUserName());
        return new SimpleAuthenticationInfo(user.getLoginName(), user.getPassword(), getName());
      }
    }
    return null;
  }

}
</code></pre><p>　存储session进mongodb的Repository和实现：</p>
<pre><code>public interface ShiroSessionRepository {

  /**
   * 
   * @param session
   */
  void saveSession(Session session);
  ......
}
</code></pre><p> MongoDBSessionRepository.java</p>
<pre><code>public class MongoDBSessionRepository implements ShiroSessionRepository {
    private MongoTemplate mongoTemplate;

    public MongoDBSessionRepository() {}

    public MongoDBSessionRepository(MongoTemplate mongoTemplate) {
        this.mongoTemplate = mongoTemplate;
    }
    @Override
    public void saveSession(Session session) {
      if (session == null || session.getId() == null) {
        return;
      }
      SessionBean bean = new SessionBean();
      bean.setKey(getSessionKey(session.getId()));
      bean.setValue(SerializeUtil.serialize(session));
      bean.setPrincipal(null);
      bean.setHost(session.getHost());
      bean.setStartTimestamp(session.getStartTimestamp());
      bean.setLastAccessTime(session.getLastAccessTime());
      bean.setTimeoutTime(getTimeoutTime(session.getStartTimestamp(), session.getTimeout()));
      mongoTemplate.insert(bean);
    }
    ......
}
</code></pre><p>　ShiroSessionDAO.java</p>
<pre><code>public class ShiroSessionDAO extends AbstractSessionDAO {

 /**
  * 日志记录器
  */
 private static final Logger log = LoggerFactory.getLogger(ShiroSessionDAO.class);

 /**
  * 数据库存储
  */
 private ShiroSessionRepository shiroSessionRepository;

 /**
  * 
  * @return
  */
 public ShiroSessionRepository getShiroSessionRepository() {
   return shiroSessionRepository;
 }

 /**
  * 
  * @param shiroSessionRepository
  */
 public void setShiroSessionRepository(ShiroSessionRepository shiroSessionRepository) {
   this.shiroSessionRepository = shiroSessionRepository;
 }

 @Override
 public void update(Session session) throws UnknownSessionException {
   getShiroSessionRepository().updateSession(session);
 }

 @Override
 public void delete(Session session) {
   if (session == null) {
     log.error(&quot;session can not be null,delete failed&quot;);
     return;
   }
   Serializable id = session.getId();
   if (id != null) {
     getShiroSessionRepository().deleteSession(id);
   }
 }

 @Override
 public Collection&lt;Session&gt; getActiveSessions() {
   return getShiroSessionRepository().getAllSessions();
 }

 @Override
 protected Serializable doCreate(Session session) {
   Serializable sessionId = this.generateSessionId(session);
   this.assignSessionId(session, sessionId);
   getShiroSessionRepository().saveSession(session);
   return sessionId;
 }

 @Override
 protected Session doReadSession(Serializable sessionId) {
   return getShiroSessionRepository().getSession(sessionId);
 }

}
</code></pre><p>　OK！所有基础类已经完成，最后写一个config用来全部初始化和配置Shiro</p>
<pre><code>@Configuration
public class ShiroConfig {
  @Resource
  private MongoTemplate mongoTemplate;
  @Resource
  private SystemUserService systemUserService;// 这是用来判断用户名和密码的service

  @Bean
  public ShiroFilterFactoryBean shiroFilter(DefaultWebSecurityManager securityManager) {
    ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean();

    shiroFilterFactoryBean.setSecurityManager(securityManager);
    shiroFilterFactoryBean.setLoginUrl(&quot;/login&quot;);
    shiroFilterFactoryBean.setSuccessUrl(&quot;/index&quot;);
    shiroFilterFactoryBean.setUnauthorizedUrl(&quot;/403&quot;);

    // 拦截器.
    Map&lt;String, String&gt; filterChainDefinitionMap = new LinkedHashMap&lt;String, String&gt;();
    filterChainDefinitionMap.put(&quot;/static/**&quot;, &quot;anon&quot;);
    filterChainDefinitionMap.put(&quot;/ajaxLogin&quot;, &quot;anon&quot;);
    filterChainDefinitionMap.put(&quot;/libs/**&quot;, &quot;anon&quot;);
    filterChainDefinitionMap.put(&quot;/images/**&quot;, &quot;anon&quot;);
    filterChainDefinitionMap.put(&quot;/logout&quot;, &quot;logout&quot;);
    filterChainDefinitionMap.put(&quot;/**&quot;, &quot;authc&quot;);

    shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap);
    return shiroFilterFactoryBean;
  }

  public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(
      DefaultWebSecurityManager securityManager) {
    AuthorizationAttributeSourceAdvisor adv = new AuthorizationAttributeSourceAdvisor();
    adv.setSecurityManager(securityManager);
    return adv;
  }

  @Bean
  public DefaultWebSecurityManager securityManager(DefaultWebSessionManager sessionManager,
      ShiroDbRealm myShiroRealm) {
    DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager();
    // 设置realm.
    securityManager.setRealm(myShiroRealm);
    securityManager.setSessionManager(sessionManager);
    return securityManager;
  }

  /**
   * 身份认证realm; (这里传递systemUserService给自定义的ShiroDbRealm初始化)
   * 
   * @return
   */
  @Bean
  public ShiroDbRealm myShiroRealm() {
    ShiroDbRealm myShiroRealm = new ShiroDbRealm(systemUserService);
    return myShiroRealm;
  }

  @Bean
  public DefaultWebSessionManager sessionManager(ShiroSessionDAO shiroSessionDao) {
    DefaultWebSessionManager sessionManager = new DefaultWebSessionManager();
    sessionManager.setGlobalSessionTimeout(1800000l);
    sessionManager.setDeleteInvalidSessions(true);
    sessionManager.setSessionValidationSchedulerEnabled(true);
    sessionManager.setSessionDAO(shiroSessionDao);
    sessionManager.setSessionIdCookieEnabled(true);
    SimpleCookie cookie = new SimpleCookie(ShiroHttpSession.DEFAULT_SESSION_ID_NAME);
    cookie.setHttpOnly(true);
    cookie.setMaxAge(1800000);
    sessionManager.setSessionIdCookie(cookie);
    return sessionManager;
  }

  @Bean
  public ShiroSessionDAO shiroSessionDao(MongoDBSessionRepository shiroSessionRepository) {
    ShiroSessionDAO dao = new ShiroSessionDAO();
    dao.setShiroSessionRepository(shiroSessionRepository);
    return dao;
  }

  @Bean
  MongoDBSessionRepository shiroSessionRepository() {
    MongoDBSessionRepository resp = new MongoDBSessionRepository(mongoTemplate);
    return resp;
  }


}
</code></pre><p>　大功告成，这里只是一个简单的配置，代码也是我从项目里面节选和修改过的，至于在controller里面怎么使用，怎么做不同权限的鉴权工作那就在自己的代码里面实现就行。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.tianshangkun.com/2017/07/12/Centos下搭建MariaDB集群/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="天狼武士">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天狼武士的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/07/12/Centos下搭建MariaDB集群/" itemprop="url">
                  Centos下搭建MariaDB集群
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-12T22:27:31+08:00">
                2017-07-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>　最近公司测试库改建，之前测试库上面装的是老的MySql，我顺手就把它换成了MariaDB，后来因为连的人太多就爆了Too Many Connections，本来很简单的事情，就让技术部一小伙子上去把连接数调大了一点，很简单，但是后来我发现他居然直接写在了My.cnf下面，显然他是不知道MariaDB早就从My.cnf下面扩展到了my.cnf.d文件夹下面，于是就跟他一番讲解，然后就引出了配置集群的话题，所以觉得有必要写一下MariaDB的一些配置和集群的配置方法。</p>
<p>　MariaDB本身就是MySql的一个分支，所以很多东西都是MySql延展下来的，既然要说的话就先从安装说起吧，在CentOS下安装还是很简单的，官网有很明确的说明以及各种安装方法，我个人比较喜欢yum源的安装方法，下面我们开始。</p>
<h2 id="第一步、安装MariaDB"><a href="#第一步、安装MariaDB" class="headerlink" title="第一步、安装MariaDB"></a>第一步、安装MariaDB</h2><p>　我们以最小集群来说，两台服务器先做主从配置，然后再做互为主从的配置,两台主机的IP分别为：192.168.70.135，192.168.70.137。</p>
<p>　先去官网上复制一下源的配置：</p>
<pre><code>https://downloads.mariadb.org/mariadb/repositories/#mirror=neusoft&amp;distro=CentOS&amp;distro_release=centos7-amd64--centos7&amp;version=10.2
</code></pre><p>　选择对应的操作系统版本及MariaDB的版本后会给出对应的配置，我这里选择的是Centos7,MariaDB的版本是10.2，所以给出的源是这样的：</p>
<pre><code># MariaDB 10.2 CentOS repository list - created 2017-07-12 14:25 UTC
# http://downloads.mariadb.org/mariadb/repositories/
[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.2/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1
</code></pre><p>　OK！然后直接vim  /etc/yum.repos.d/MariaDB.repo复制进去保存即可，然后yum安装</p>
<pre><code>yum install MariaDB-server MariaDB-client
</code></pre><p>　不要以为安装完成就可以使用，mysql的用户和数据文件夹先指定一下，我的数据文件放在了/opt/data/mysql下面，执行下面的命令</p>
<pre><code>mysql_install_db --defaults-file=/etc/my.cnf --datadir=/opt/data/mysql/ --user=mysql
</code></pre><p>　OK!这时启动一下看看</p>
<pre><code>systemctl start mariadb
systemctl status mariadb
</code></pre><p>　不出意外，启动成功，设置一下root密码</p>
<pre><code>mysqladmin -u root password &quot;123456&quot;
</code></pre><p>　登录一下，不出意外应该成功了。</p>
<h2 id="第二步、配置MariaDB"><a href="#第二步、配置MariaDB" class="headerlink" title="第二步、配置MariaDB"></a>第二步、配置MariaDB</h2><p>　上面只是简单的安装，下面我们再简单的配置一下为集群做准备，先进入/etc/my.cnf.d/文件夹，MariaDB的所有配置文件都在这下面，主要配置文件还是在/etc/my.cnf.d/server.cnf下面</p>
<pre><code>vim /etc/my.cnf.d/server.cnf
</code></pre><p>　修改[mysqld]下面的内容</p>
<pre><code>[mysqld]
character-set-server=utf8
lower_case_table_names=1
init_connect=&apos;SET NAMES utf8&apos;
datadir=/opt/data/mysql
socket=/opt/data/mysql/mysql.sock
server-id   = 1
log-bin=mysql-bin
log-bin-index=master-bin.index
relay-log=relay-log
relay-log-index=relay-bin
log-slave=updates
</code></pre><p>　1.  character-set-server是设置数据库的编码格式<br>　2.  lower_case_table_names是设置数据库不区分大小写<br>　3.  datadir，socket是设置数据库实例的目录<br>　4.  server-id这个很重要，是指定集群中数据库服务的ID，在集群情况下每台数据库服务的ID都不能重复<br>　5.  log-bin和log-bin-index是开始MySQL的二进制日志并指定日志文件名<br>　6.  relay-log和relay-log-index是开始MySQL的中继日志并指定日志文件名<br>　7.  log-slave这个是设定slave节点的二进制输出，若没有设定此项则slave不会输出二进制，但是为了能够让slave也能够升级为master则该项最好配置一下</p>
<p>　为了能让两台MySQL服务能够互相连接，最好先创建让他们互相连接的数据库账户，现在master上简历用户，并授权，该账户为同步专用</p>
<pre><code>mysql&gt; GRANT REPLICATION SLAVE ON *.* TO repl@192.168.70.137 IDENTIFIED BY &apos;123456&apos;;
</code></pre><p>　PS：另外一台机器的/etc/my.cnf.d/server.cnf中其他配置都一样，就是server-id改成2，配置同步账户的时候IP地址改成可访问的机器的IP就行。</p>
<p>  配置完成后两台机器都重启。</p>
<h2 id="第三步、准备同步-配置主从"><a href="#第三步、准备同步-配置主从" class="headerlink" title="第三步、准备同步,配置主从"></a>第三步、准备同步,配置主从</h2><p>　全部配置并且两台机器都启动成功之后，就可以开始准备同步两台机器的数据了，在此之前先登录第一台机器的数据库，查看一下master的状态：</p>
<pre><code>mysql&gt; show master status;
+------------------+----------+--------------+------------------+
| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+------------------+----------+--------------+------------------+
| mysql-bin.000001 |      922 |              |                  |
+------------------+----------+--------------+------------------+
</code></pre><p>　File就是当前该数据库binlog日志的文件索引所在的文件名，position是当前日志索引的位置，将这两个值记录下来，然后登录第二台数据库服务器，启动slave：</p>
<pre><code>mysql&gt; CHANGE MASTER TO 
        MASTER_HOST=&apos;192.168.70.135&apos;,
        MASTER_USER=&apos;repl&apos;,
        MASTER_PASSWORD=&apos;123456&apos;,
        MASTER_LOG_FILE=&apos;mysql-bin.000001&apos;,
        MASTER_LOG_POS=922;
</code></pre><p>　执行完成之后如果没有报错再执行show slave status\G命令查看slave节点的同步状态，如果ERROR没有显示错误并且Slave_IO_Running都为yes则表示同步成功。</p>
<pre><code>    MariaDB [(none)]&gt; show slave status\G;
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: 192.168.70.135
                  Master_User: repl
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysql-bin.000001
          Read_Master_Log_Pos: 922
               Relay_Log_File: relay-bin.000003
                Relay_Log_Pos: 555
        Relay_Master_Log_File: mysql-bin.000001
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
              Replicate_Do_DB: 
          Replicate_Ignore_DB: 
           Replicate_Do_Table: 
       Replicate_Ignore_Table: 
      Replicate_Wild_Do_Table: 
  Replicate_Wild_Ignore_Table: 
                   Last_Errno: 0
                   Last_Error: 
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 922
              Relay_Log_Space: 858
              Until_Condition: None
               Until_Log_File: 
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File: 
           Master_SSL_CA_Path: 
              Master_SSL_Cert: 
            Master_SSL_Cipher: 
               Master_SSL_Key: 
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error: 
               Last_SQL_Errno: 0
               Last_SQL_Error: 
  Replicate_Ignore_Server_Ids: 
             Master_Server_Id: 1
               Master_SSL_Crl: 
           Master_SSL_Crlpath: 
                   Using_Gtid: No
                  Gtid_IO_Pos: 
      Replicate_Do_Domain_Ids: 
  Replicate_Ignore_Domain_Ids: 
                Parallel_Mode: conservative
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it
1 row in set (0.00 sec)
</code></pre><p>　这时在master节点的数据库服务器中创建数据库、创建表的话slave节点就直接可以看到了</p>
<p>　PS：同步之前最好先检查一下你服务器的防火墙有没有拦截3306端口！</p>
<h2 id="第四步、双主配置"><a href="#第四步、双主配置" class="headerlink" title="第四步、双主配置"></a>第四步、双主配置</h2><p>　前面的所有步骤完成后已经可以搭建单主多slave的架构了，如果想配置互为主从的其实很简单，在master节点中重复第三步，把master的IP指向对应的节点就好，不过需要注意的是Master可以有多个Slave，但是一个Slave只能挂靠一个Master！好的，我们先去192.168.70.137这台节点上查看一下master的status(就是第三步中的slave节点)</p>
<pre><code>mysql&gt; show master status;
+------------------+----------+--------------+------------------+
| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+------------------+----------+--------------+------------------+
| mysql-bin.000002 |      985 |              |                  |
+------------------+----------+--------------+------------------+
</code></pre><p>　然后再刚才master节点的数据库中执行：</p>
<pre><code>mysql&gt; CHANGE MASTER TO 
        MASTER_HOST=&apos;192.168.70.137&apos;,
        MASTER_USER=&apos;repl&apos;,
        MASTER_PASSWORD=&apos;123456&apos;,
        MASTER_LOG_FILE=&apos;mysql-bin.000002&apos;,
        MASTER_LOG_POS=985;
</code></pre><p>　如果没有报错，那么双主执行成功，在两台机器间创建数据库、创建表、创建数据测试一下，两台机器就会互相同步数据了！</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.tianshangkun.com/2017/06/13/Centos下docker搭建Hadoop集群/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="天狼武士">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天狼武士的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/06/13/Centos下docker搭建Hadoop集群/" itemprop="url">
                  Centos下docker搭建Hadoop集群
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-13T15:56:26+08:00">
                2017-06-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>　随着业务发展需要，数据量的逐步提升，需要用到Hadoop来处理一些数据，所以搭建一个Hadoop集群，搭建Hadoop集群需要多台主机，但是由于资源有限，所以刚好可以利用近些年比较火的docker来搭建，用docker搭建也有一个好处，就是一次构建多节点重复利用。在上服务器之前，现在自己的电脑里用虚拟机模拟一下，顺便可以做个记录，把可能遇到的坑先趟过去（谁叫我笔记本16G内存呢）。</p>
<h2 id="第一步、安装Centos，部署docker"><a href="#第一步、安装Centos，部署docker" class="headerlink" title="第一步、安装Centos，部署docker"></a>第一步、安装Centos，部署docker</h2><p>　先从Centos官网载一个最小版的Centos7镜像，用VMWare安装，由于是最小版镜像所以装完之后有很多组件需要手动yum安装，（什么？为什么不装完全版的？我要是有几台刀片我也装啊！）</p>
<p>　完成之后的第一步当然是换yum源：</p>
<pre><code>mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup

wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
</code></pre><p>　安装完之后记得 yum makecache一下。由于是Minimal版，连最基本的网络工具都没有，所以全都开始手动安装：</p>
<pre><code>yum install -y wget
yum install -y gcc
</code></pre><p>　安装基本工具之后就是安装docker,docker安装很简单直接yum install -y docker，docker基本配置和使用就不再赘述了，docker的镜像本身下载很慢的，所以这里我用的是daocloud的加速器，那下载速度杠杠的！但是不知道daocloud最近怎么了，curl生成的加速器是有问题的，配置了之后docker启动不了，看了日志发现是他们修改的etc/docker/daemon.json有问题，里面多了一个逗号，把逗号去了就行。 接下来就是启动docker</p>
<pre><code>systemctl start docker
</code></pre><p>　没毛病，直接启动成功，接卸来下载镜像</p>
<pre><code>docker pull daocloud.io/centos:6
</code></pre><p>　为什么不用centos7而是用6，这个问题就比较搞笑了，刚开始用的就是7的镜像但是有个很严重的BUG就是systemd无法使用，也就是说容器内的服务是无法用systemctl管理的，网上也有解决方法，但是都不好使！所以退而求其次用centos6就行。镜像拉取完成之后启动镜像：</p>
<pre><code>docker run -it -h master --name master daocloud.io/library/centos:6 /bin/bash
</code></pre><p>　ok~!镜像启动成功，控制台会直接进入镜像内的控制台，如果想退回宿主机控制台直接ctrl+p ctrl+q可以从容器返回宿主机，再想回来直接docker attach 【容器名称或ID】。</p>
<h2 id="第二步、制作Hadoop镜像"><a href="#第二步、制作Hadoop镜像" class="headerlink" title="第二步、制作Hadoop镜像"></a>第二步、制作Hadoop镜像</h2><p>　进入容器控制台，开始搭建Hadoop，在搭建之前一样要先把基础工具给装了，Hadoop是JAVA写的所以JDK先给装上：</p>
<pre><code>wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz

mkdir /usr/java

tar -zxvf jdk-8u131-linux-x64.tar.gz -C /usr/java
</code></pre><p>　修改环境变量</p>
<pre><code>vim /etc/profile
#在最下方加入JAVA配置
export PATH USER LOGNAME MAIL HOSTNAME HISTSIZE HISTCONTROL
export JAVA_HOME=/usr/java/jdk1.8.0_131
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
#source一下
source /etc/profile
</code></pre><p>　安装gcc,vim,lrzsz和ssh</p>
<pre><code>yum install -y gcc
yum install -y vim
yum install -y lrzsz
yum -y install openssh-server
yum -y install openssh-clients
</code></pre><p>　配置ssh免密登录</p>
<pre><code>ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre><p>　关闭烦人sellinux:</p>
<pre><code>setenforce 0
</code></pre><p>　启动ssh</p>
<pre><code>service sshd start
</code></pre><p>　测试一下</p>
<pre><code>ssh master
</code></pre><p>　如果没什么问题就代表一些准备就绪。</p>
<p>　完成之后开始下载Hadoop镜像：</p>
<pre><code>wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.8.0/hadoop-2.8.0.tar.gz
mkdir /usr/local/hadoop
tar -zxvf hadoop-2.8.0.tar.gz -C /usr/local/hadoop
</code></pre><p>　配置环境变量</p>
<pre><code>vim /etc/profile
#在最下方加入Hadoop配置
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.8.0
export PATH=$PATH:$HADOOP_HOME/bin
</code></pre><p>　修改Hadoop的配置文件，进入Hadoop的目录</p>
<pre><code>cd /usr/local/hadoop/hadoop-2.8.0/etc/hadoop/
</code></pre><p>　在hadoop-env.sh 和 yarn-env.sh 在开头添加JAVA环境变量JAVA_HOME</p>
<p>　修改hadoop core-site.xml文件</p>
<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://master:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;io.file.buffer.size&lt;/name&gt;
        &lt;value&gt;131702&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;file:/home/tsk/hadoop-2.8.0/tmp&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><p>　修改hdfs-site.xml文件</p>
<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
        &lt;value&gt;file:/home/tsk/hadoop-2.8.0/dfs/name&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
        &lt;value&gt;file:/home/tsk/hadoop-2.8.0/dfs/data&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;2&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
        &lt;value&gt;master:9001&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><p>　修改mapred-site.xml文件</p>
<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
        &lt;value&gt;master:10020&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
        &lt;value&gt;master:19888&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><p>　修改yarn-site.xml</p>
<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
        &lt;value&gt;master:8032&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
        &lt;value&gt;master:8030&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
        &lt;value&gt;master:8031&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
        &lt;value&gt;master:8033&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
        &lt;value&gt;master:8088&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
        &lt;value&gt;1024&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><p>　由于我准备配置三个salve节点所以在slaves文件中添加</p>
<pre><code>slave1
slave2
slave3
</code></pre><p>　完成之后尝试一下</p>
<pre><code>ldd /usr/local/hadoop/hadoop-2.8.0/lib/native/libhadoop.so.1.0.0
</code></pre><p>　这时提示GLIBC_2.14 required，centos6的源最高到2.12，这里需要2.14，所以只能手动make安装</p>
<pre><code>wget http://ftp.gnu.org/gnu/glibc/glibc-2.14.tar.gz
tar zxvf glibc-2.14.tar.gz
cd glibc-2.14
mkdir build
cd build
../configure --prefix=/usr/local/glibc-2.14
make
make install
ln -sf /usr/local/glibc-2.14/lib/libc-2.14.so /lib64/libc.so.6
</code></pre><p>　完成之后再ldd就没有问题了！接下来就是构建上面做的所有操作，将其变成一个镜像以便复用，先Ctrl+p和Ctrl+q返回宿主机控制台然后输入命令：</p>
<pre><code>docker commit master tsk/hadoop
</code></pre><p>　等一会之后会发现镜像只做完成docker images一下就能看到自己只做的镜像了。</p>
<h2 id="第三步、启动镜像"><a href="#第三步、启动镜像" class="headerlink" title="第三步、启动镜像"></a>第三步、启动镜像</h2><p>　先配置docker的网络，给每台机器配置host</p>
<pre><code>docker inspect --format=&apos;{{.NetworkSettings.IPAddress}}&apos; master
</code></pre><p>　接下来逐个启动镜像：</p>
<pre><code>docker stop master
docker rm master
docker run -it -p 50070:50070 -p 19888:19888 -p 8088:8088 -h master --name master tsk/hadoop /bin/bash
docker run -it -h slave1 --name slave1 tsk/hadoop /bin/bash
docker run -it -h slave2 --name slave2 tsk/hadoop /bin/bash
docker run -it -h slave3 --name slave3 tsk/hadoop /bin/bash
</code></pre><p>　然后attach到每个节点上面source一下配置hosts，启动sshd，完成之后开始准备启动Hadoop</p>
<pre><code>hadoop namenode -format
/usr/local/hadoop/hadoop-2.8.0/sbin/start-all.sh
</code></pre><p>　最后要做的事情就是等待<del>~等待</del><del>~等待</del>~~~~！直到全部启动完成！完成之后用浏览器访问虚拟机IP地址:50070，如果没有任何问题的话你就可以看到如下画面，至此Hadoop集群搭建成功！</p>
<p><img src="/images/hadoop.png"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.tianshangkun.com/2017/05/04/获取我附近的商店方法-二-GeoHash算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="天狼武士">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天狼武士的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/04/获取我附近的商店方法-二-GeoHash算法/" itemprop="url">
                  获取我附近的商店方法(二):GeoHash算法
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-05-04T17:03:29+08:00">
                2017-05-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>　上一篇提到根据坐标获取附近的商店根据半径计算的方法，这一篇说一下目前比较常用的GeoHash方法，先简单介绍一下GeoHash算法：</p>
<h3 id="认识GeoHash"><a href="#认识GeoHash" class="headerlink" title="认识GeoHash"></a>认识GeoHash</h3><p>　GeoHash算法将二维经纬度坐标直接转换成字符串，每一个字符串代表一个矩形区域，也就是说，这个矩形区域内所有的点（经纬度坐标）都共享相同的GeoHash字符串，字符串的长度越大，矩形的区域就越小，经度也就越高。字符串相似的表示距离相近，这样可以利用字符串的前缀匹配来查询附近的POI信息</p>
<h3 id="GeoHash算法的步骤"><a href="#GeoHash算法的步骤" class="headerlink" title="GeoHash算法的步骤"></a>GeoHash算法的步骤</h3><p>　地球纬度区间是[-90,90]，经度区间是[-180,180]，通过区间法对经度和纬度分别进行计算，假如我们获取到的当前坐标为经度-0.12866, 纬度38.534413，以纬度为例：</p>
<ol>
<li>将纬度平均分成两个区间：[-90,0),[0,90]，成为左区间和右区间，可以判断出38.534413属于右区间，则值为1,（如果属于左区间则值为0）；</li>
<li>接着将右区间继续划分，就变成了[0,45),[45,90],此时，38.534413属于左区间，则值为0</li>
<li>递归上述过程，则区间的值会越来越逼近38.534413</li>
<li>随着算法的进行，我们将会得到一个序列，序列的长度跟递归的次数有关，但是一定要保证的是经度和纬度的序列长度是一样的，我这里设置的递归长度是30，经度和纬度加起来就是60，</li>
<li>根据算法我们最终得到经度的序列为[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0]，纬度的序列为[1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0]，然后我们根据此序列再组合一个新的序列偶数位放经度，奇数位放纬度，把2串编码组合生成新串[0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0]，实际上这个序列是一串二进制</li>
<li>最后，我们将这个新串转换成十进制再使用0-9、b-z（去掉a, i, l, o）对这组十进制进行编码得到的字符串eyzgjnfr4p0p就是最终的GeoHash编码。下图是网上给出的不同编码长度给出的精度：</li>
</ol>
<p><img src="/images/geoHash.png"><br><img src="/images/geoHashLength.png"></p>
<p>　得到GeoHash的值之后可以同样的保存近数据库中，每次查询离我最近的数据的时候理论上来说可以根据精确度截取GeoHash的值进行模糊查询，但是这样查询是有问题的，因为你没法保证你每次查询时你的当前坐标刚好在这个矩形的正中间，如果你的坐标处在矩形的边界，那么你就无法获取其附近的数据，那么这个问题怎么解决呢？其实也很简单，以你当前所在位置的矩形为九宫格的中间格子，再获取其相邻的8个矩形。</p>
<p>　获取其他8个区域的GeoHash也很简单，上面的表已经给出了每一个区域内经度和纬度的宽度，那么直接加减后就可以得出周边相邻的8个格子，我这里自己写了一套GeoHash的算法，得出当前坐标的GeoHash的值以及相邻8个格子的值，代码比较多就不贴上来了，在GitHub上有一个别人写好的JAVA版本的可以直接拿来用，叫<a href="https://github.com/kungfoo/geohash-java" target="_blank" rel="external">geohash-java</a>,貌似maven的仓库中也有</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;ch.hsr&lt;/groupId&gt;
    &lt;artifactId&gt;geohash&lt;/artifactId&gt;
    &lt;version&gt;1.3.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><p> 使用的方法也很简单</p>
<pre><code>GeoHash geoHash = GeoHash.withCharacterPrecision(38.534413, -0.12866, 12);
System.out.println(geoHash.toBase32());//获取当前的GeoHash的值
GeoHash[] adjacent = geoHash.getAdjacent();//获取整个九宫格的GeoHash的值
for (GeoHash hash : adjacent) {
  System.out.println(hash.toBase32());
}
</code></pre><p> 那么获取到整个九宫格之后至于查询和排序就简单多了，这里不再赘述。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.tianshangkun.com/2017/05/04/获取我附近的商店方法-一-根据坐标计算半径/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="天狼武士">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天狼武士的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/04/获取我附近的商店方法-一-根据坐标计算半径/" itemprop="url">
                  获取我附近的商店方法(一):根据坐标计算半径
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-05-04T16:30:34+08:00">
                2017-05-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>　现在所有的电商类APP都会有根据我的位置获取我附近的商户、离我最近的商户的筛选排序功能，那么这个功能是怎么实现的呢？显然单纯的数据库查询很难做到，一般数据库中只会存储商户的经度和纬度的坐标，那么根据这两个坐标获，再根据用户手机端定位到的经纬度坐标获经过计算得出离我最近的商户，这种实现方式可以有两种：第一种根据坐标及需要的距离计算出半径获取；第二种是通过geoHash算法进行转换。</p>
<p>　咱们并不讨论这两种方式的优缺点及具体的可行性问题，但从理论上说明一下两种方式的实现过程，这篇文章先说一下根据半径获取的方式：</p>
<p>　假定我们数据库中每个商户都有一个自己的地理位置坐标（经度：longitude；纬度：latitude），这两个参数可以精确到一个点，然后以这个点为圆心再根据需要的距离计算出最大的半径，代码如下：</p>
<pre><code>/**
* 地图坐标BEAN
* 
* @author sk.tian
* 
*/
public class LocationBean {
    private Double longitude; // 经度
    private Double latitude; // 纬度
    private int raidus; // 半径
    private Double minLat; // 最小纬度
    private Double maxLat; // 最大纬度
    private Double minLng; // 最小经度
    private Double maxLng; // 最大经度

    public Double getLongitude() {
        return longitude;
    }

    public void setLongitude(Double longitude) {
        this.longitude = longitude;
    }

    public Double getLatitude() {
        return latitude;
    }

    public void setLatitude(Double latitude) {
        this.latitude = latitude;
    }

    public int getRaidus() {
        return raidus;
    }

    public void setRaidus(int raidus) {
        this.raidus = raidus;
    }

    public Double getMinLat() {
        return minLat;
    }

    public void setMinLat(Double minLat) {
        this.minLat = minLat;
    }

    public Double getMaxLat() {
        return maxLat;
    }

    public void setMaxLat(Double maxLat) {
        this.maxLat = maxLat;
    }

    public Double getMinLng() {
        return minLng;
    }

    public void setMinLng(Double minLng) {
        this.minLng = minLng;
    }

    public Double getMaxLng() {
        return maxLng;
    }

    public void setMaxLng(Double maxLng) {
        this.maxLng = maxLng;
    }

}

/**
 * 根据坐标计算半径多少米范围之内的最大、最小经度和纬度
 * 
 * @param longitude
 *            经度
 * @param latitude
 *            纬度
 * @param raidus
 *            半径（米）
 * @return
 */
public static LocationBean loadCirle(Double longitude, Double latitude,
        int raidus) {
    LocationBean bean = new LocationBean();
    bean.setLongitude(longitude);
    bean.setLatitude(latitude);
    bean.setRaidus(raidus);
    Double degree = (24901 * 1609) / 360.0;
    double raidusMile = raidus;

    Double dpmLat = 1 / degree;
    Double radiusLat = dpmLat * raidusMile;
    Double minLat = latitude - radiusLat;
    Double maxLat = latitude + radiusLat;

    Double mpdLng = degree * Math.cos(latitude * (Math.PI / 180));
    Double dpmLng = 1 / mpdLng;
    Double radiusLng = dpmLng * raidusMile;
    Double minLng = longitude - radiusLng;
    Double maxLng = longitude + radiusLng;
    bean.setMaxLat(maxLat);
    bean.setMaxLng(maxLng);
    bean.setMinLat(minLat);
    bean.setMinLng(minLng);
    return bean;
}
</code></pre><p>  这个loadCirle方法就是计算出最大和最小经纬度的，将结果封装近LocationBean中，获取到locationBean之后，数据库查询的时候最大和最小的经纬度查询即可：</p>
<pre><code>select * from shops where latitude &gt;=#{minLat} and latitude&lt;=#{maxLat} and longitude&gt;=#{minLng} and longitude&lt;=#{maxLng}
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.tianshangkun.com/2017/04/26/转-奥瑞旅行是什么鬼？/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="天狼武士">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天狼武士的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/26/转-奥瑞旅行是什么鬼？/" itemprop="url">
                  转:奥瑞旅行是什么鬼？
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-26T10:38:10+08:00">
                2017-04-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="/images/aoorey1.jpg" class="full-image"></p>
<h2 id="以下文章截取自与奥瑞旅行CEO对其产品的一次不成熟的描述："><a href="#以下文章截取自与奥瑞旅行CEO对其产品的一次不成熟的描述：" class="headerlink" title="以下文章截取自与奥瑞旅行CEO对其产品的一次不成熟的描述："></a>以下文章截取自与奥瑞旅行CEO对其产品的一次不成熟的描述：</h2><blockquote>
<p><strong>奥瑞旅行：</strong>你出过国吗？从你起心动念到踏上异国他乡的土地再到回来，都需要花钱对吗？<br><strong>假装用户：</strong>是的，所有动作全部伴随着花钱。无论买旅游产品还是在境外消费（买东西或者吃顿饭）都得花钱。<br><strong>奥瑞旅行：</strong>但是你在境外花钱爽吗？花的值吗？<br><strong>假装用户：</strong>不爽是肯定的，国内啥都手机在线支付，习惯了；国外花钱太原始了，至于值不值的不好说。<br><strong>奥瑞旅行：</strong>没错，如果你在国外本来就要买这个东西，恰好现在有个软件，能让你买的时候打折，付款的时候手机快捷支付，你用不用？不仅如此，奥瑞旅行这个怪物除了给你提供最细致的旅游服务外，还要让你清楚、明白、实惠、便捷的在境外花好每一笔钱。<br><strong>假装用户：</strong>还有这么好的软件？请问哪里能下载？什么时候可以用？<br><strong>奥瑞旅行：</strong>别着急，今年6月以后去英国玩耍的小伙伴就可以率先用到啦！年底去欧洲玩耍的小伙伴也可以用啦。有没用很期待，心痒痒的？另外，奥瑞旅行的1.0版本海外中文接机、送机和包车定制服务也依然为你守候，专人专车从机场接你送到酒店，再从酒店送你去shopping，之后满载而归送回酒店。好土豪的样子有木有？！</p>
</blockquote>
<p>　　如文中描述，奥瑞旅行这款产品实际上是打破了中国人出境消费的很多弊端问题，比如目前中国人出境消费，要么使用现金，要么使用VISA、master card或银联卡等银行卡；使用现金的方式出境是收到金融管制的，就是你一次性可以带的货币数量是有限的，而使用银行卡要么换算成当地货币，要么转换成美金支付，很不方便，且毫无任何折扣可言，信用卡的话回来还钱也是个问题，经常会有人忘记还其他货币的钱而被银行纳入信用不良的人群之中。</p>
<p>　　之前我也说过中国的互联网发展速度是相当的快的，目前中国人直接跳过了信用卡时代步入了移动支付的时代，但仅限于中国，现在有这么一款产品直接将中国人的消费习惯带入国外，让中国人不管身处何处都可以享受到国内的移动支付便捷，这无疑是一个很不错的想法。</p>
<p>　　</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.tianshangkun.com/2017/04/20/angularJS创建一个上传照片的指令/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="天狼武士">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天狼武士的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/20/angularJS创建一个上传照片的指令/" itemprop="url">
                  angularJS创建一个上传照片的指令
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-20T19:48:33+08:00">
                2017-04-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>　angularJS在近几年发展火热，也无疑是目前市面上来说比较牛逼且成熟的框架，在单页面前端应用当中应该可以说是王者，双向绑定省去了大量的前端代码，控制器在其作用于方面的控制也是相当腻害，今天我们要说的是另外一个比较牛逼的功能，就是angularJS的指令directive，之前没听说过angularJS指令的朋友请自行度娘，随便搜一条文章都比我说的详细，这次用一个我自己写的图片上传的指令来做为案例，详细说明一下实际操作过程中的指令。</p>
<p>　之前我们前端的附件上传用的是jqueryFileUpload，每次使用都要在页面把样式画好，然后再控制器中初始化upload组件，上传成功或失败时候还要做相应的处理，这样每次写一个附件上传都要写代码去处理，这样很重复劳动，所以就想利用angularJS的指令把重复劳动的环节给去掉，具体代码如下：</p>
<pre><code>.directive(&apos;imageUpload&apos;,[&apos;Constants&apos;,function(Constants){
return {
    restrict: &apos;E&apos;,
    scope: {
        scopeModel:&apos;=&apos;,
        title:&apos;@&apos;
    },
    template : &apos;&lt;fieldset&gt;&apos;
            +&apos;&lt;legend&gt;{{title}}&lt;span class=&quot;fileinput-button&quot;&gt;&lt;span&gt;重新上传&lt;/span&gt;&apos;
            +&apos;&lt;input type=&quot;file&quot; name=&quot;file&quot;&gt;&lt;/span&gt;&lt;/legend&gt;&apos;
            + &apos;&lt;span class=&quot;profile-picture&quot;&gt;&apos;
            + &apos;&lt;img class=&quot;img-responsive&quot; alt=&quot;{{title}}&quot; ng-src=&quot;{{loadImg(scopeModel)}}&quot; style=&quot;display: block;&quot;/&gt;&apos;
            + &apos;&lt;/fieldset&gt;&apos;,
    link : function(scope, element, attrs) {

        $(element).fileupload({
            url: &apos;file/upload&apos;,
            dataType: &apos;json&apos;,
            done: function(e, data) {
                var res = data.result;
                if(res.success){
                    scope.scopeModel=res.data.fileKey;
                    scope.$apply();
                }
            }
        });
        scope.loadImg=function(key){
            if(undefined==scope.scopeModel || null==scope.scopeModel || scope.scopeModel===&apos;&apos;){
                return $.ctx+&apos;/images/noImage.jpg&apos;;
            }
            if(scope.scopeModel.indexOf(&apos;http://&apos;)&gt;-1){
                return scope.scopeModel;
            }
            return $.ctx+&apos;/file/getFile?fileKey=&apos;+scope.scopeModel;
        }
    }
};
</code></pre><p>}]);</p>
<p>　指令完成之后在前端页面上只需要写一行代码就可以完成照片的加载（如果是修改页面需要加载出原照片）和上传功能，其中scopeModel是用来双向绑定的，在调用的时候把controller中的model传递进去之后可以实现指令和controller之间的双向绑定，代码中的template为element模板，可以根据具体的样式自己更换（我用的是bootstrap），使用如下：</p>
<pre><code>&lt;image-upload scope-model=&quot;imagePath&quot; title=&quot;照片上传&quot;&gt;&lt;/image-upload&gt;
</code></pre><p>　PS：代码为我实际业务代码中截取和删减过后的精简版。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.tianshangkun.com/2017/04/20/easyUI与angularJS整合（2）：表单提交篇/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="天狼武士">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="天狼武士的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/20/easyUI与angularJS整合（2）：表单提交篇/" itemprop="url">
                  easyUI与angularJS整合（2）：表单提交篇
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-20T14:53:33+08:00">
                2017-04-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>　我们知道easyUI的form表单提供了submit方法，可以直接提交表单的内容至服务器，默认的提交方式是form data,通过表单内元素的name和value作为键值对提交，但是如果你后端用的是springMVC的话，那默认他是无法接收到的，会返回HTTP415的错误，原因是POST提交数据的方法springMVC不接受，这个时候要么你在springMVC的上下文中加入支持form fata提交的配置，要么修改表单POST中content-type，都可以解决。</p>
<p>　在这里我选择的是直接改前端的代码，因为之前一直用的angularJS，数据提交都是用的angularJS的方法，springMVC已经配置过支持application/json的提交方式，所以直接修改content-type为”application/json;charset=UTF-8”的话会更好一点，但是看了easyUI的源码之后就知道，easyUI的form中是没有application/json;charset=UTF-8这种方式提交的，这个时候你就需要扩展easyUI了，顺便可以扩展一下easyUI的form中缺少的获取表单数据的方法（easyUI的form表单没有提供获取表单内数据的方法）。</p>
<p>　具体操作是这样的，我们可以通过$.extend方法对easyUI各种组件进行重写或者扩展，现在我们尝试一下扩展其form组件，增加一个获取表单内数据的方法getObjectData，让这个方法返回我们一个指定表单内的数据，以object类型返回：</p>
<pre><code>$.extend($.fn.form.methods, {  
getObjectData:function(jq, options){
    var target =jq[0];
    var data={};
    for (i = 0; i &lt; target.length; i++) {
        if(target[i].name &amp;&amp; target[i].getAttribute(&apos;disabled&apos;) === null){
            data[target[i].name]=target[i].value;
        }
        //form[i].name, form[i].value
    }
    return data;
}});
</code></pre><p>　这样就可以在需要的时候直接使用</p>
<pre><code>var data=$(&apos;#systemMenuEditForm&apos;).form(&apos;getObjectData&apos;);
</code></pre><p>　获取到表单数据之后就可以不用form的submit方法可以换成任何你想用的方法，鉴于我使用的了angularJS，所以直接用angularJS的$http.post方法提交就行。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/1.jpg"
               alt="天狼武士" />
          <p class="site-author-name" itemprop="name">天狼武士</p>
           
              <p class="site-description motion-element" itemprop="description">天狼武士的个人博客</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">13</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">27</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/1171031951" target="_blank" title="新浪微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  新浪微博
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">天狼武士</span>
</div>


<div class="powered-by">
  <a class="theme-link" href="http://www.tianshangkun.com">天狼武士</a> 专属博客
</div>

<div class="theme-info">
  苏ICP备 17021575
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  








  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

</body>
</html>
